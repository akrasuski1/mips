In this work we implemented and benchmarked three different MIPS 
approximation algorithms
against the Faiss-provided optimized full search and IVF algorithm.
From our tests, we see that quantization-based and ALSH algorithms
are not performing too well --- at least on datasets we chose.
Quantization algorithm is quite predictable in speed --- it performed
comparably, or even worse than full search on all large benchmarks.
On the other hand, ALSH gives results dependent on dataset --- for
two of three datasets we used, its precision was too low to be useful.\footnote{
Except for parameter choices which gave runtime slower than full search,
which were not pictured on graphs. These results are useless too, of course.}
Further research might be needed to find dataset features causing
ALSH to perform well.

The clear winner here is the Hierarchical $K$-Means algorithm. It was consistently
giving better results than other algorithms, including Faiss' IVF. It can be also
highly parametrized, allowing wide range of points on time-precision tradeoff,
including speedups a couple times higher than the best we could get from IVF.

Our contribution to this algorithm, theoretical derivation of optimal cluster sizes,
is a generalization of $K$-Means, often giving faster results than the original algorithm
for the same level of precision, especially in high-precision parts of the
speed-precision tradeoff.

In our benchmarks, we used only integral values of $n_{train}$, which set
a natural lower bound of this parameter to 1. There is no reason why this
parameter, determining tree shape, could not be set to even lower real values.
Extrapolating from our tests, we would expect to achieve even higher speedups
(probably with some small precision loss).

[TODO] Weź ktoś napisz o tym że są fajne zastosowania, @marcin, że można w
pythonie (łatwo!) przyspieszać i że jesteśmy fajni... \remark{No właśnie. Napiszcie to :)}
