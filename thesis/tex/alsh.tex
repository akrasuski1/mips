The Asymmetric Locality Sensitive Hashing (ALSH) \cite{alsh} algorithm depends on hash functions
in order to determine similarities between vectors themselves and between vectors and queries.
Hash function transforms its possibly long input such as file (or vector of numbers like in our case)
into much smaller domain of output values.
A collision happens when two different inputs cause the function to return the same output.
In this algorithm's case it is a desired property for vectors to collide if they are in some way similar to each other.
\par
Single function may not be able to distingiush similarities among all vectors correctly i.e. some
significantly differing vectors might collide. Because of that $K$ functions grouped into
meta-hash function are used in this approach. Each function detects similarities of various aspects of vectors.
As these functions are partially random, aspects they will represent are not deterministic.
It is assumed that meta-hash function is better suited to discover in what ways vectors
resemble one another.
After a vector is hashed by all $K$ functions, outputs taken together form a hash table key.
An entry in hash table consists of key pointing to a set of vector indices for which the meta-hash function
returned this exact key. In other words, vectors are placed in ``buckets'' ---
every bucket represents one value of meta-hash and consequently contains similar vectors, where
similarity is more precisely evaluated than one single hash function would evaluate.
To allow even higher abstraction level, $L$ meta-hash functions can be used to create $L$ hash tables ---
vectors will be placed in $L$ sets of buckets according to some rules depending on applied meta-hash function.
Before hashing, vectors can be augmented using one of approaches presented in the beginning
of this section, e.g. Shrivastrava's or Neyshabur's transformation.
\par
Searching the index with query can as well be preceded by its earlier transformation.
Detecting which database vectors are most similar to query relies on checking with which
vectors the query collided after being hashed using exactly the same meta-hash functions.
Speaking the bucket metaphor: the bucket which contains the query, is also likely to contain
vectors we are interested in.
Vectors' indices acquired this way are collected from every hash table and form candidate set from which top-$k$
most similar vectors should be chosen. In order to do that, exhaustive search on these candidates
can be performed.
Another idea is to create a ranking sorted by number of tables in which a collision
between query and vector occurred.
\par
Meta-hashes $H$ are concatenations of ordinary hashes $h$ --- so $j^{th}$ vector will be placed in 
$i^{th}$ table in bucket\footnote{
In actual implementation, the sequence of hashes is compressed to a single 64-bit integer. This
marginally increases probability of unwanted collision, but improves speed and memory consumption.
}:

$$H_{i,j} = [h_{1,i}(x_j), h_{2,i}(x_j), \dots h_K(x_j)]$$

The following equation describes implemented hash function.
$N$ denotes Gaussian distribution and $U$ denotes random uniform distribution.
\begin{equation*}
h_{i,j}(x) = \floor*{\frac{a_{i,j}^T x + b_{i,j}}{r}}, a_{i,j} \sim N(0,1), b_{i,j} \sim U(0,r)
\end{equation*}
\par
Random vectors $a_{i,j}$ and scalars $b_{i,j}$ are generated once and used throughout whole
algorithm, including the querying phase. Parameters specific to this algorithm are $L$, $K$ and $r$.

\begin{algorithm}
	\caption{ALSH clustering}
	\begin{algorithmic}
		\For{$l = 0, 1, \dots, L$}
			\For{$k = 0, 1, \dots, K$}
				\State Key[$k$] $\gets$ Hash($k, l, x$)
			\EndFor
			\State HashTables[$l$][Key].Vectors $\gets x$
		\EndFor
		\State \Return HashTables
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{ALSH querying}
	\begin{algorithmic}
		\State Result $\gets \emptyset$
		\For{$l = 0, 1, \dots, L$}
			\For{$k = 0, 1, \dots, K$}
				\State Key[$k$] $\gets$ Hash($k, l, q$)
			\EndFor
			\State Result $\gets$ Result $\cup$ HashTables[$l$][Key].Vectors
		\EndFor
		\State \Return Result
	\end{algorithmic}
\end{algorithm}

Vector multiplication is regarded as an elementary operation.
The hash function we use does not do much more than that (except one addition,
one division and one rounding),
so whole hash function can be treated as elementary operation.
The index construction phase has complexity of $O(n \cdot K \cdot L)$,
because every of $n$ database vectors
has to be hashed $K$ times in every of $L$ hash tables.
The querying phase complexity is hard to estimate, since it is data-dependent.
The lower bound is $\Omega(K \cdot L)$, as this is the number of hashes to be
calculated. Adding results to set can potentially take $O(n)$ time in worst
case, if every vector collides with query. This is an unlikely scenario if
hashing function is chosen properly. The pseudocode as written returns a set
of candidates --- if we want to get top-K of them, we need to calculate exact
inner product of query with each of them --- this again can take up to
$O(n)$ time. Careful choosing of $K$ and $L$ constants may avoid the issue
by making collisions only reasaonably frequent.


\subsubsection*{Example}
Preparation of $L = 2$ hash tables for a database having 4 vectors:
$x_1, x_2, x_3$ and $x_4$ is presented below.
First, vectors' keys in both tables are computed and based on them hash tables are created:
\begin{gather*}
H_{1,1} = [h_{1,1}(x_1), h_{2,1}(x_1), h_{3,1}(x_1)] = [0, 1, 0]\\
H_{1,2} = [h_{1,1}(x_2), h_{2,1}(x_2), h_{3,1}(x_2)] = [-1, 1, 0]\\
H_{1,3} = [h_{1,1}(x_3), h_{2,1}(x_3), h_{3,1}(x_3)] = [1, 0, -1]\\
H_{1,4} = [h_{1,1}(x_4), h_{2,1}(x_4), h_{3,1}(x_4)] = [0, 1, 0]\\
H_{2,1} = [h_{1,2}(x_1), h_{2,2}(x_1), h_{3,2}(x_1)] = [2, 1, 0]\\
H_{2,2} = [h_{1,2}(x_2), h_{2,2}(x_2), h_{3,2}(x_2)] = [2, 1, 0]\\
H_{2,3} = [h_{1,2}(x_3), h_{2,2}(x_3), h_{3,2}(x_3)] = [2, 1, 0]\\
H_{2,4} = [h_{1,2}(x_4), h_{2,2}(x_4), h_{3,2}(x_4)] = [1, 0, -1]
\end{gather*}
\renewcommand{\arraystretch}{1.2}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\multicolumn{2}{|c|}{$L = 1$} \\
\hline
$\bm{[0, 1, 0]}$ & $\bm{\{1, 4\}}$ \\
\hline
$[-1, 1, 0]$ & $\{2\}$ \\
\hline
$[1, 0, -1]$ & $\{3\}$ \\
\hline
\end{tabular}
$\ \ \ \ $
\begin{tabular}{|l|c|}
\hline
\multicolumn{2}{|c|}{$L = 2$} \\
\hline
$[2, 1, 0]$ & $\{1, 2, 3\}$ \\
\hline
$\bm{[1, 0, -1]}$ & $\bm{\{4\}}$ \\
\hline
\end{tabular}
\end{center}

Before looking for vectors that match query $q$, it has to be hashed as well:
\begin{gather*}
H_{1,q} = [h_{1,1}(q), h_{2,1}(q), h_{3,1}(q)] = \bm{[0, 1, 0]} \\
H_{2,q} = [h_{1,2}(q), h_{2,2}(q), h_{3,2}(q)] = \bm{[1, 0, -1]} \\
\end{gather*}

$H_{1,q}$ i $H_{2,q}$ are the keys with which one should look for vectors in first and second hash table respectively (printed in bold).
In the example, $x_1$ and $x_4$ turned out to be similar to $q$ according to first table, but only $x_4$ appears to be similar to query according to second table.
If the collision ranking idea is used, $x_4$ is the most similar vector to query, because the collision happened in both tables.
\newpage
