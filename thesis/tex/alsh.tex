The Asymmetric Locality Sensitive Hashing (ALSH) \cite{alsh} algorithm depends on hash functions
in order to determine similarities between vectors themselves and between vectors and queries.
A hash function transforms its possibly long input such as file (or vector of numbers like in our case)
into much smaller domain of output values.
A collision happens when two different inputs cause the function to return the same output.
In the considered case it is a desired property for vectors to collide if they are in some way similar to each other.

A single function may not be able to distingiush similarities among all vectors correctly, i.e., some
significantly different vectors might collide. Because of that $K$ functions grouped into
meta-hash function are used in this approach. Each function detects similarities of various aspects of vectors.
As these functions are partially random, aspects they will represent are not deterministic.
It is assumed that the meta-hash function is better suited to discover in what ways vectors
resemble one another.
After a vector is hashed by all $K$ functions, outputs taken together form a hash table key.
An entry in the hash table consists of a key pointing to a set of vector indices for which the meta-hash function
returned this exact key. In other words, vectors are placed in ``buckets'' ---
every bucket represents one value of meta-hash and consequently contains similar vectors, where
similarity is more precisely evaluated than one single hash function would evaluate.
To allow even higher abstraction level, $L$ meta-hash functions can be used to create $L$ hash tables ---
vectors will be placed in $L$ sets of buckets according to some rules depending on the applied meta-hash function.
Before hashing, vectors can be augmented using one of the approaches presented in the beginning
of this chapter, e.g., SL or NS transformation. ALSH index construction is shown in Algorithm~\ref{alg:alsh-cluster}.

\begin{algorithm}
	\caption{ALSH clustering}
	\begin{algorithmic}
		\For{$l = 0, 1, \dots, L$}
			\For{$k = 0, 1, \dots, K$}
				\State Key[$k$] $\gets$ Hash($k, l, x$)
			\EndFor
			\State HashTables[$l$][Key].Vectors $\gets x$
		\EndFor
		\State \Return HashTables
	\end{algorithmic}
\label{alg:alsh-cluster}
\end{algorithm}

Searching the index with query (Algorithm~\ref{alg:alsh-query}) can as well be preceded by its earlier transformation.
Detecting which database vectors are most similar to query relies on checking with which
vectors the query collided after being hashed using exactly the same meta-hash functions.
Speaking the bucket metaphor: the bucket which contains the query, is also likely to contain
vectors we are interested in.
Vectors' indices acquired this way are collected from every hash table and form candidate set from which top-$k$
most similar vectors should be chosen. In order to do that, exhaustive search on these candidates
can be performed.
Another idea is to create a ranking sorted by the number of tables in which a collision
between query and vector occurred.

\begin{algorithm}
	\caption{ALSH querying}
	\begin{algorithmic}
		\State Result $\gets \emptyset$
		\For{$l = 0, 1, \dots, L$}
			\For{$k = 0, 1, \dots, K$}
				\State Key[$k$] $\gets$ Hash($k, l, q$)
			\EndFor
			\State Result $\gets$ Result $\cup$ HashTables[$l$][Key].Vectors
		\EndFor
		\State \Return Result
	\end{algorithmic}
\label{alg:alsh-query}
\end{algorithm}

Meta-hashes $H$ are concatenations of ordinary hashes $h$ --- so the $j^{th}$ vector will be placed in 
the $i^{th}$ table in bucket:\footnote{
In the actual implementation, the sequence of hashes is compressed to a single 64-bit integer. This
marginally increases probability of unwanted collision, but improves speed and memory consumption.
}

$$H_{i,j} = [h_{1,i}(x_j), h_{2,i}(x_j), \dots h_K(x_j)]$$

The following equation describes the implemented hash function, where
$N$ denotes Gaussian distribution and $U$ denotes uniform distribution:
\begin{equation*}
h_{i,j}(x) = \floor*{\frac{a_{i,j}^T x + b_{i,j}}{r}}, a_{i,j} \sim N(0,1), b_{i,j} \sim U(0,r)
\end{equation*}
\par
Random vectors $a_{i,j}$ and scalars $b_{i,j}$ are generated once and used throughout the whole
algorithm, including the querying phase. Parameters specific to this algorithm are $L$, $K$ and $r$.

Vector multiplication is regarded as an elementary operation.
The hash function we use does not do much more than that (except one addition,
one division and one rounding),
so the whole hash function can be treated as elementary operation.
The index construction phase has complexity of $O(n \cdot K \cdot L)$,
because every of $n$ database vectors
has to be hashed $K$ times in every of $L$ hash tables.
The querying phase complexity is hard to estimate, since it is data-dependent.
The lower bound is $\Omega(K \cdot L)$, as this is the number of hashes to be
calculated. Adding results to set can potentially take $O(n)$ time in the worst
case, if every vector collides with the query. This is an unlikely scenario if
the hashing function is chosen properly. The pseudocode as presented in Algorithm~\ref{alg:alsh-query} returns a set
of candidates --- if we want to get top-$k$ of them, we need to calculate the exact
inner product of the query with each of them --- this again can take up to
$O(n)$ time. Careful choosing of $K$ and $L$ may avoid the issue
by making collisions only reasonably frequent.

To finalize the description of ALSH let us consider the following example.
We use $L = 2$ hash tables for a database of 4 vectors:
$x_1, x_2, x_3$ and $x_4$. 
First, vectors' keys in both tables are computed and based on them the hash tables are created:
\begin{eqnarray*}
H_{1,1} & = & [h_{1,1}(x_1), h_{2,1}(x_1), h_{3,1}(x_1)] = [0, 1, 0]\\
H_{1,2} & = &  [h_{1,1}(x_2), h_{2,1}(x_2), h_{3,1}(x_2)] = [-1, 1, 0]\\
H_{1,3} & = &  [h_{1,1}(x_3), h_{2,1}(x_3), h_{3,1}(x_3)] = [1, 0, -1]\\
H_{1,4} & = &  [h_{1,1}(x_4), h_{2,1}(x_4), h_{3,1}(x_4)] = [0, 1, 0]\\
H_{2,1} & = &  [h_{1,2}(x_1), h_{2,2}(x_1), h_{3,2}(x_1)] = [2, 1, 0]\\
H_{2,2} & = &  [h_{1,2}(x_2), h_{2,2}(x_2), h_{3,2}(x_2)] = [2, 1, 0]\\
H_{2,3} & = &  [h_{1,2}(x_3), h_{2,2}(x_3), h_{3,2}(x_3)] = [2, 1, 0]\\
H_{2,4} & = &  [h_{1,2}(x_4), h_{2,2}(x_4), h_{3,2}(x_4)] = [1, 0, -1]
\end{eqnarray*}
\renewcommand{\arraystretch}{1.2}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\multicolumn{2}{|c|}{$L = 1$} \\
\hline
$\bm{[0, 1, 0]}$ & $\bm{\{1, 4\}}$ \\
\hline
$[-1, 1, 0]$ & $\{2\}$ \\
\hline
$[1, 0, -1]$ & $\{3\}$ \\
\hline
\end{tabular}
$\ \ \ \ $
\begin{tabular}{|l|c|}
\hline
\multicolumn{2}{|c|}{$L = 2$} \\
\hline
$[2, 1, 0]$ & $\{1, 2, 3\}$ \\
\hline
$\bm{[1, 0, -1]}$ & $\bm{\{4\}}$ \\
\hline
\end{tabular}
\end{center}

Before looking for vectors that match query $q$, it has to be hashed as well:
\begin{eqnarray*}
H_{1,q} & = & [h_{1,1}(q), h_{2,1}(q), h_{3,1}(q)] = \bm{[0, 1, 0]} \\
H_{2,q} & = & [h_{1,2}(q), h_{2,2}(q), h_{3,2}(q)] = \bm{[1, 0, -1]} 
\end{eqnarray*}

$H_{1,q}$ i $H_{2,q}$ are the keys with which one should look for vectors in the first and second hash table respectively (printed in bold).
In the example, $x_1$ and $x_4$ turned out to be similar to $q$ according to the first table, but only $x_4$ appears to be similar to the query according to the second table.
If the collision ranking idea is used, $x_4$ is the most similar vector to $q$, because the collision happened in both tables.
