The Asymmetric Locality Sensitive Hashing \cite{alsh} algorithm builds an index based on hash tables.
First, one should divide all database vectors' components by the maximum norm of any vector.
This divisor can optionally be multiplied by a number from  $[0,1]$ interval.
After such scaling $m$ components in the form of $|x_i|^2$, $|x_i|^4$, $|x_i|^8$, $ ...$, $|x_i|^{2^m}$ should be added at the end of each vector $x_i$.
\par
The index consists of $L$ hash tables.
In order to create $i$-th table it is necessary to hash database vectors using $K$ hash functions: $h_{1,i}, h_{2,i}, ..., h_{K,i}$.
The results of hashing of vector $x_j$ will create another vector $H_{i,j} = [h_{1,i}(x_j), h_{2,i}(x_j), ..., h_{K,i}(x_j)]$. $H_{i,j}$ is the key where the number of $j$-th vector of database (in fact: $j$) should be placed in $i$-th hash table.
The following equation describes implemented hash function.
$N$ denotes Gaussian distribution and $U$ denotes random uniform distribution.
\begin{equation*}
h_{i,j}(\bm{x}) = \floor*{\frac{\bm{a_{i,j}^T x} + b_{i,j}}{r}}, \bm{a_{i,j}} \sim N(0,1), b_{i,j} \sim U(0,r)
\end{equation*}
\par
An example of preparation of $L = 2$ hash tables for a database having 4 vectors: $x_1, x_2, x_3$ and $x_4$ is presented below.
\\
Computations of vectors' keys in first table:\\
$H_{1,1} = [h_{1,1}(x_1), h_{2,1}(x_1), h_{3,1}(x_1)] = [0, 1, 0]$\\
$H_{1,2} = [h_{1,1}(x_2), h_{2,1}(x_2), h_{3,1}(x_2)] = [-1, 1, 0]$\\
$H_{1,3} = [h_{1,1}(x_3), h_{2,1}(x_3), h_{3,1}(x_3)] = [1, 0, -1]$\\
$H_{1,4} = [h_{1,1}(x_4), h_{2,1}(x_4), h_{3,1}(x_4)] = [0, 1, 0]$\\
Computations of vectors' keys in first table:\\
$H_{2,1} = [h_{1,2}(x_1), h_{2,2}(x_1), h_{3,2}(x_1)] = [2, 1, 0]$\\
$H_{2,2} = [h_{1,2}(x_2), h_{2,2}(x_2), h_{3,2}(x_2)] = [2, 1, 0]$\\
$H_{2,3} = [h_{1,2}(x_3), h_{2,2}(x_3), h_{3,2}(x_3)] = [2, 1, 0]$\\
$H_{2,4} = [h_{1,2}(x_4), h_{2,2}(x_4), h_{3,2}(x_4)] = [1, 0, -1]$\\
Hash tables will then look like this:
\begin{center}
\begin{tabular}{|l|c|}
\hline
\multicolumn{2}{|c|}{$L = 1$} \\
\hline
$[0, 1, 0]$ & $\{1, 4\}$ \\
\hline
$[-1, 1, 0]$ & $\{2\}$ \\
\hline
$[1, 0, -1]$ & $\{3\}$ \\
\hline
\end{tabular}
\begin{tabular}{|l|c|}
\hline
\multicolumn{2}{|c|}{$L = 2$} \\
\hline
$[2, 1, 0]$ & $\{1, 2, 3\}$ \\
\hline
$[1, 0, -1]$ & $\{4\}$ \\
\hline
\end{tabular}
\end{center}
Searching the index with query $q$ requires an earlier preparation of said query.
$m$ components equal to $\frac{1}{2}$ should be added at the end of $q$.
Then the query is hashed in a similar manner to database vectors -- in each table it is hashed $K$ times with corresponding functions.
An example process of hashing the query is given below.\\
$H_{1,q} = [h_{1,1}(q), h_{2,1}(q), h_{3,1}(q)] = [0, 1, 0]$\\
$H_{2,q} = [h_{1,2}(q), h_{2,2}(q), h_{3,2}(q)] = [1, 0, -1]$

$H_{1,q}$ i $H_{2,q}$ are the keys with which one should look for vectors in first and second hash table respectively.
Found vectors are supposed to show similarity to $q$.
\par
$x_1$ and $x_4$ turned out to be similar to $q$ according to first table, but only $x_4$ appears to be similar to query according to second table.
In real-world scenarios, a ranking of vectors is made that is sorted by number of tables in which a collision between query and vector occurred.
Ultimately, $x_4$ is the most similar vector to query, because the collision happened in both tables.
