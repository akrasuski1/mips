The quantization-based inner product search algorithm \cite{quantization}
instead of looking at whole vectors and queries like other two algorithms described in thesis,
looks for similarities between parts of vectors.
One can view it as trying to match queries to vectors independently for every part and
by using this knowledge, estimating how well queries will match whole vectors.
The parts are not vectors simply cut into pieces but they are prepared randomly
 --- all vectors and queries are permuted (the order of vectors' components changed)
by a fixed permutation.
\par
Each vector should be then mapped into $K$ subspaces, i.e. divided into $K$ parts with equal number of components.
Let us denote the set of $i^{th}$ parts of all vectors in database by $P_i$.
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|}
\multicolumn{1}{r}{} & \multicolumn{1}{c}{$P_{1}$} & \multicolumn{1}{c}{$P_{2}$}
& \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$P_{K}$} \\
\hhline{~----}
$x_1$ = & $p_{1,1}$ & $p_{1,2}$ & $\cdots$ & $p_{1,K}$ \\
\hhline{~----}
$x_2$ = & $p_{2,1}$ & $p_{2,2}$ & $\cdots$ & $p_{2,K}$ \\
\hhline{~----}
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
\hhline{~----}
$x_n$ = & $p_{n,1}$ & $p_{n,2}$ & $\cdots$ & $p_{n,K}$ \\
\hhline{~----}
\end{tabular}
\end{center}

\begin{equation*}
P_i = \{ p_{1,i}, p_{2,i}, \ldots, p_{n,i} \}
\end{equation*}

The essential step in algorithm is to perform $k$-means procedure on every set $P_i$.
This gives $k$ centroids $C_{i,j}$ and assignments $c_{i,l}$ of parts in $P_i$ to these centroids.
Every set has got its own independent centroids, which can be different from other sets' centroids.

\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|}
\multicolumn{1}{r}{} & \multicolumn{1}{c}{$P_{1}$} & \multicolumn{1}{c}{$P_{2}$}
& \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$P_{K}$} \\
\hhline{~----}
$x_1$ & $c_{1,1}$ & $c_{2,1}$ & $\cdots$ & $c_{K,1}$ \\
\hhline{~----}
$x_2$ & $c_{1,2}$ & $c_{2,2}$ & $\cdots$ & $c_{K,2}$ \\
\hhline{~----}
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
\hhline{~----}
$x_n$ & $c_{1,n}$ & $c_{2,n}$ & $\cdots$ & $c_{K,n}$ \\
\hhline{~----}
\end{tabular}
\end{center}

\begin{equation*}
\forall_i \forall_l\ \  c_{i,l} \in \{1,2,...,k\}
\end{equation*}

$k$-means clustering mentioned above means that vectors are quantized at the level of parts
and whole vector is represented by approximations of its parts.
\par
The score of vector (how related it is to query) is computed as sum of inner products
between query's parts and centroids assigned to respective parts of vector.
Naive approach would calculate these inner products seperately for each vector.
As many vectors' parts are approximated by the same centroids, it is enough to compute
these products once and use them when needed.
Because of that a table $T$ having $K$ columns and $k$ rows has to be created.
Table entry $T_{i,j}$ is a dot product between $i^{th}$ part of query $q$ ($q_i$) and $j^{th}$
centroid assigned to set $P_i$ ($C_{i,j}$).

\renewcommand{\arraystretch}{1.6}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\multicolumn{1}{c}{$P_{1}$} & \multicolumn{1}{c}{$P_{2}$}
& \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$P_{K}$} \\
\hhline{----}
$ q_1^T C_{1,1} $ & $ q_2^T C_{2,1} $ & $ \cdots $ & $ q_K^T C_{K,1} $\\
\hhline{----}
$ q_1^T C_{1,2} $ & $ q_2^T C_{2,2} $ & $ \cdots $ & $ q_K^T C_{K,2} $\\
\hhline{----}
$ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $\\
\hhline{----}
$ q_1^T C_{1,k} $ & $ q_2^T C_{2,k} $ & $ \cdots $ & $ q_K^T C_{K,k} $\\
\hhline{----}
\end{tabular}
\end{center}

Inner product approximation for vector $x_l$ is then
$$\sum_{i=1}^{K} T_{i,c_{i,l}} = \sum_{i=1}^{K} q_i^T C_{i,c_{i,l}}$$
In the end, certain number of vectors with highest approximations is reported.
For each vector $x_l$, $l = 1,2,...,n$, the squared error arising from clustering can be expressed this way:
$$ \sum_{i=1}^{K}  [q_{i}^{T} (C_{i, c_{i,l}} - p_{l,i})]^2$$
