The quantization-based inner product search algorithm \cite{quantization}
instead of looking at whole vectors and queries like the other two algorithms described in this thesis \remark{na razie opisany był jeden algorytm; warto rozważyć zmianę kolejności podrozdziałów},
looks for similarities between parts of vectors.
One can view it as trying to match queries to vectors independently for every part and
by using this knowledge, estimating how well queries will match whole vectors.
Before anything else, all vectors and queries are randomly permuted 
(i.e., the order of vectors' components is changed)
by a fixed permutation.\footnote{
Intuitively, this is done to spread the variance of vectors evenly into each slice
(another way of doing this is to apply a random rotation of each vector). Formal
explanation why this is needed is in the original paper.
}

Each vector should be then mapped into $K$ subspaces, i.e. divided into $K$ parts with a equal number of components.
Let us denote the set of the $i^{th}$ parts of all vectors in database by $P_i$.
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|}
\multicolumn{1}{r}{} & \multicolumn{1}{c}{$P_{1}$} & \multicolumn{1}{c}{$P_{2}$}
& \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$P_{K}$} \\
\hhline{~----}
$x_1$ = & $p_{1,1}$ & $p_{1,2}$ & $\cdots$ & $p_{1,K}$ \\
\hhline{~----}
$x_2$ = & $p_{2,1}$ & $p_{2,2}$ & $\cdots$ & $p_{2,K}$ \\
\hhline{~----}
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
\hhline{~----}
$x_n$ = & $p_{n,1}$ & $p_{n,2}$ & $\cdots$ & $p_{n,K}$ \\
\hhline{~----}
\end{tabular}
\end{center}

\begin{equation*}
P_i = \{ p_{1,i}, p_{2,i}, \ldots, p_{n,i} \}
\end{equation*}

The essential step in the algorithm is to perform $k$-means \remark{małe czy duże $k$?} procedure on every set $P_i$.
This gives $k$ centroids $C_{i,j}$ and assignments $A_{i,l}$ of parts in $P_i$ to these centroids.
Every set has got its own independent centroids, which can be different from the other sets' centroids.

\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|}
\multicolumn{1}{r}{} & \multicolumn{1}{c}{$A_{1}$} & \multicolumn{1}{c}{$A_{2}$}
& \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$A_{K}$} \\
\hhline{~----}
$x_1$ & $A_{1,1}$ & $A_{2,1}$ & $\cdots$ & $A_{K,1}$ \\
\hhline{~----}
$x_2$ & $A_{1,2}$ & $A_{2,2}$ & $\cdots$ & $A_{K,2}$ \\
\hhline{~----}
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
\hhline{~----}
$x_n$ & $A_{1,n}$ & $A_{2,n}$ & $\cdots$ & $A_{K,n}$ \\
\hhline{~----}
\end{tabular}
\end{center}

\begin{equation*}
\forall_i \forall_l\ \  A_{i,l} \in \{1,2,...,k\}
\end{equation*}

$k$-means clustering mentioned above means that vectors are quantized at the level of parts
and the whole vector is represented by approximations of its parts.

The score of a vector (how related it is to query) is computed as a sum of inner products
between query's parts and centroids assigned to corresponding parts of the vector.
Naive approach would calculate these inner products seperately for each vector,
but since there are less centroids than vectors, it is enough to compute the products once
and use them when needed.
For this, an auxilliary table $T$ having $K$ columns and $k$ rows has to be created.
Table entry $T_{i,j}$ is a dot product between the $i^{th}$ part of query $q$ ($q_i$) and the $j^{th}$
centroid assigned to set $P_i$ ($C_{i,j}$).

\renewcommand{\arraystretch}{1.6}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hhline{----}
$ q_1^T C_{1,1} $ & $ q_2^T C_{2,1} $ & $ \cdots $ & $ q_K^T C_{K,1} $\\
\hhline{----}
$ q_1^T C_{1,2} $ & $ q_2^T C_{2,2} $ & $ \cdots $ & $ q_K^T C_{K,2} $\\
\hhline{----}
$ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $\\
\hhline{----}
$ q_1^T C_{1,k} $ & $ q_2^T C_{2,k} $ & $ \cdots $ & $ q_K^T C_{K,k} $\\
\hhline{----}
\end{tabular}
\end{center}

The inner product approximation for vector $x_l$ is then:
$$q \cdot x_l \approx \sum_{i=1}^{K} T_{i,A_{i,l}} = \sum_{i=1}^{K} q_i^T C_{i,A_{i,l}} \,.$$
In the end, certain number of vectors with highest approximations is reported.
For each vector $x_l$, $l = 1,2,...,n$, squared error arising from clustering can be expressed in the following way:
$$ \sum_{i=1}^{K}  [q_{i}^{T} (C_{i, A_{i,l}} - p_{l,i})]^2$$

\begin{algorithm}
	\caption{Quantization-based clustering}
	\begin{algorithmic}
		\For{$i = 0, 1, \dots, K$}
			\State $P_i = \emptyset$
		\EndFor
		\For{$l = 0, 1, \dots, n$}
			\State $x_l' \gets$ Permute($x_l$)
			\State $part[0], part[1], \dots, part[K] \gets$ MakeParts($x_l'$)
			\For{$i = 0, 1, \dots, K$}
				\State $P_i = P_i \cup part[i]$
			\EndFor
		\EndFor
		\For{$i = 0, 1, \dots, K$}
			\State $C_i = \{C_{i,1}, C_{i,2}, \dots, C_{i,k}\}, A_i = \{A_{i,1}, A_{i,2}, \dots, A_{i,n}\} \gets$ Kmeans($P_i, K$)
		\EndFor
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Quantization-based querying}
	\begin{algorithmic}
		\State $q' \gets$ Permute($q$)
		\State $q_0, q_1, \dots, q_K \gets$ MakeParts($q'$)
		\For{$i = 0, 1, \dots, K$}
			\For{$j = 0, 1, \dots, k$}
				\State $T_{i,j} = q_i^T C_{i,j}$
			\EndFor
		\EndFor
		\For{$l = 0, 1, \dots, n$}
			\State Result[$l$] $\gets 0$
			\For{$i = 0, 1, \dots, K$}
				\State Result[$l$] $\gets$ Result[$l$]$+ T_{i,A_{i,l}}$
			\EndFor
		\EndFor
		\State \Return $\argmax_{l}$ Result[$l$]
	\end{algorithmic}
\end{algorithm}

The runtime of this query algorithm is a sum of time taken for building the $T$ table and calculating the inner product of each vector with the query using $T$ as a lookup table.
The former does $O(K \cdot k)$ inner products, but each of those is only $O(\frac{d}{K})$
elements long (where $d$ is the number of dimensions of original vectors). 
Multiplying these, we get $O(k \cdot d)$ floating-point multiplications.
The second loop takes $O(n \cdot K)$ floating-point {additions}. Additions are often
cheaper than multiplications, but that does not change the asymptotics.

The total runtime is thus $O(k \cdot d + n \cdot K)$, but the number of centroids $k$ is usually 
relatively small --- if smaller than $\frac{n \cdot K}{d}$, the second term dominates 
and the expression can be simplified to
$O(K \cdot n)$. This is still linear in $n$ like full search, but has a smaller multiplicative
term, leading to theoretical $O(\frac{d}{K})$ speedup (with possible extra speedup from changing
multiplication to addition). \remark{Oznaczenia!!! Wcześniej liczba przykładów była oznaczona przez $N$}

\remark{Warto dodać ten sam przykład co we wcześniejszym podrozdziale}
