The quantization-based inner product search algorithm \cite{quantization}
looks for similarities at the level of vectors' parts -- unlike two other algorithms
described in this thesis which do this by considering vectors as whole.
These parts are not vectors simply cut into pieces but they are produced in a slightly random way
 -- all vectors and queries are permuted (the order of vectors' components changes) randomly.
This permutation is fixed (the same for all vectors \textit{and} queries).
Each vector should be then mapped into $K$ subspaces, i.e. divided into $K$ parts with equal number of components.
Let us denote the set of $i^{th}$ parts of all vectors in database by $P_i$.
For each set $P_i$, $k$-means procedure is performed giving $k$ centroids $C_{i,j}$ and assignments of parts in $P_i$ to these centroids.
It means that vectors are quantized at the level of parts and whole vector is represented as
approximations of its parts.
\par
The score of vector (how similar it is to query) is computed as sum of inner products
between query's parts and centroids assigned to respective parts of vector.
Naive approach would compute these inner products seperately for each vector.
As many vectors' parts are approximated by the same centroids, it is enough to compute
these products once and use them when needed.
Because of that a table $T$ having $K$ columns and $k$ rows has to be created.
Table entry $T_{i,j}$ is a dot product between $i^{th}$ part of query $q$ ($q_i$) and $j^{th}$
centroid assigned to set $P_i$ ($C_{i,j}$).
Inner product approximation for vector $x_l$ is then
$$\sum_{i=1}^{K} T_{i,c_{i,l}}$$ where $c_{i,l}$ is the centroid assigned to $x_l$ in part $i$.
In the end certain number of vector with highest approximations is reported.

\par
Consider database of $n = 5$ vectors already divided into $K = 4$ parts:
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|}
\hhline{~----}
$x_1$ = & $p_{1,1}$ & $p_{1,2}$ & $p_{1,3}$ & $p_{1,4}$ \\
\hhline{~----}
$x_2$ = & $p_{2,1}$ & $p_{2,2}$ & $p_{2,3}$ & $p_{2,4}$ \\
\hhline{~----}
$x_3$ = & $p_{3,1}$ & $p_{3,2}$ & $p_{3,3}$ & $p_{3,4}$ \\
\hhline{~----}
$x_4$ = & $p_{4,1}$ & $p_{4,2}$ & $p_{4,3}$ & $p_{4,4}$ \\
\hhline{~----}
$x_5$ = & $p_{5,1}$ & $p_{5,2}$ & $p_{5,3}$ & $p_{5,4}$ \\
\hhline{~----}
\end{tabular}
$\ \ \ \ \ P_i = \{ p_{1,i}, p_{2,i}, \ldots, p_{n,i} \}$ \\
\end{center}

\smallskip

Let us assume that each set $P_i$ (column) has been clustered to $k=2$ centroids ($C_{1,i}, C_{2,i}, i \in \{1,2,3,4\}$) as shown below. Note that clusters are independent in each set.
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|}
\hhline{~----}
$x_1$ & $C_{2,1}$ & $C_{1,2}$ & $C_{2,3}$ & $C_{1,4}$ \\
\hhline{~----}
$x_2$ & $C_{1,1}$ & $C_{1,2}$ & $C_{2,3}$ & $C_{1,4}$ \\
\hhline{~----}
$x_3$ & $C_{2,1}$ & $C_{2,2}$ & $C_{2,3}$ & $C_{1,4}$ \\
\hhline{~----}
$x_4$ & $C_{1,1}$ & $C_{2,2}$ & $C_{1,3}$ & $C_{2,4}$ \\
\hhline{~----}
$x_5$ & $C_{1,1}$ & $C_{1,2}$ & $C_{2,3}$ & $C_{2,4}$ \\
\hhline{~----}
\end{tabular}
\end{center}

\smallskip

After dividing query into parts $q_1, q_2, \ldots, q_K$ the pre-computed table $T$ will look as follows:
\renewcommand{\arraystretch}{1.6}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$ q_1^T C_{1,1} $ & $ q_2^T C_{1,2} $ & $ q_3^T C_{1,3} $ & $ q_4^T C_{1,4} $\\
\hline
$ q_1^T C_{2,1} $ & $ q_2^T C_{2,2} $ & $ q_3^T C_{2,3} $ & $ q_4^T C_{2,4} $\\
\hline
\end{tabular}
\end{center}

\smallskip

Based on table $T$ is it easy to compute inner product approximations, e.g.:
\begin{gather*}
 q^T x_1 \approx q_1^T C_{2,1} + q_2^T C_{1,2} + q_3^T C_{2,3} + q_4^T C_{1,4} = T_{1,2} + T_{2,1} + T_{3,2} + T_{4,1} \\
 q^T x_2 \approx q_1^T C_{1,1} + q_2^T C_{1,2} + q_3^T C_{2,3} + q_4^T C_{1,4} = T_{1,1} + T_{2,1} + T_{3,2} + T_{4,1} \\
 q^T x_3 \approx q_1^T C_{2,1} + q_2^T C_{2,2} + q_3^T C_{2,3} + q_4^T C_{1,4} = T_{1,2} + T_{2,2} + T_{3,2} + T_{4,1} \\
 q^T x_4 \approx q_1^T C_{1,1} + q_2^T C_{2,2} + q_3^T C_{1,3} + q_4^T C_{2,4} = T_{1,1} + T_{2,2} + T_{3,1} + T_{4,2} \\
 q^T x_5 \approx q_1^T C_{1,1} + q_2^T C_{1,2} + q_3^T C_{2,3} + q_4^T C_{2,4} = T_{1,1} + T_{2,1} + T_{3,2} + T_{4,2}
\end{gather*}

The squared error arising from clustering can be expressed this way:
$$ \sum_{i=1}^{K}  [q_{i}^{T} (C_{i, c_{i,l}} - p_{l,i})]^2, l = 1, 2, \ldots, n$$
