The quantization-based inner product search algorithm \cite{quantization}
instead of looking at whole vectors and queries like other two algorithms described in thesis,
looks for similarities between parts of vectors.
One can view it as trying to match queries to vectors independently for every part and
by using this knowledge, estimating how well queries will match whole vectors.
Before anything else, all vectors and queries are randomly permuted 
(the order of vectors' components changed)
by a fixed permutation\footnote{
Intuitively, this is done to spread the variance of vectors evenly into each slice
(another way of doing this is to apply a random rotation of each vector). Formal
explanation why this is needed is in the original paper.
}.
\par
Each vector should be then mapped into $K$ subspaces, i.e. divided into $K$ parts with equal number of components.
Let us denote the set of $i^{th}$ parts of all vectors in database by $P_i$.
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|}
\multicolumn{1}{r}{} & \multicolumn{1}{c}{$P_{1}$} & \multicolumn{1}{c}{$P_{2}$}
& \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$P_{K}$} \\
\hhline{~----}
$x_1$ = & $p_{1,1}$ & $p_{1,2}$ & $\cdots$ & $p_{1,K}$ \\
\hhline{~----}
$x_2$ = & $p_{2,1}$ & $p_{2,2}$ & $\cdots$ & $p_{2,K}$ \\
\hhline{~----}
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
\hhline{~----}
$x_n$ = & $p_{n,1}$ & $p_{n,2}$ & $\cdots$ & $p_{n,K}$ \\
\hhline{~----}
\end{tabular}
\end{center}

\begin{equation*}
P_i = \{ p_{1,i}, p_{2,i}, \ldots, p_{n,i} \}
\end{equation*}

The essential step in algorithm is to perform $k$-means procedure on every set $P_i$.
This gives $k$ centroids $C_{i,j}$ and assignments $A_{i,l}$ of parts in $P_i$ to these centroids.
Every set has got its own independent centroids, which can be different from other sets' centroids.

\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|c|c|c|c|}
\multicolumn{1}{r}{} & \multicolumn{1}{c}{$A_{1}$} & \multicolumn{1}{c}{$A_{2}$}
& \multicolumn{1}{c}{$\cdots$} & \multicolumn{1}{c}{$A_{K}$} \\
\hhline{~----}
$x_1$ & $A_{1,1}$ & $A_{2,1}$ & $\cdots$ & $A_{K,1}$ \\
\hhline{~----}
$x_2$ & $A_{1,2}$ & $A_{2,2}$ & $\cdots$ & $A_{K,2}$ \\
\hhline{~----}
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
\hhline{~----}
$x_n$ & $A_{1,n}$ & $A_{2,n}$ & $\cdots$ & $A_{K,n}$ \\
\hhline{~----}
\end{tabular}
\end{center}

\begin{equation*}
\forall_i \forall_l\ \  A_{i,l} \in \{1,2,...,k\}
\end{equation*}

$k$-means clustering mentioned above means that vectors are quantized at the level of parts
and whole vector is represented by approximations of its parts.
\par
The score of a vector (how related it is to query) is computed as sum of inner products
between query's parts and centroids assigned to corresponding parts of vector.
Naive approach would calculate these inner products seperately for each vector,
but since there are less centroids than vectors, it is enough to compute the products once
and use them when needed.
For this, an auxilliary table $T$ having $K$ columns and $k$ rows has to be created.
Table entry $T_{i,j}$ is a dot product between $i^{th}$ part of query $q$ ($q_i$) and $j^{th}$
centroid assigned to set $P_i$ ($C_{i,j}$).

\renewcommand{\arraystretch}{1.6}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hhline{----}
$ q_1^T C_{1,1} $ & $ q_2^T C_{2,1} $ & $ \cdots $ & $ q_K^T C_{K,1} $\\
\hhline{----}
$ q_1^T C_{1,2} $ & $ q_2^T C_{2,2} $ & $ \cdots $ & $ q_K^T C_{K,2} $\\
\hhline{----}
$ \vdots $ & $ \vdots $ & $ \ddots $ & $ \vdots $\\
\hhline{----}
$ q_1^T C_{1,k} $ & $ q_2^T C_{2,k} $ & $ \cdots $ & $ q_K^T C_{K,k} $\\
\hhline{----}
\end{tabular}
\end{center}

Inner product approximation for vector $x_l$ is then:
$$q \cdot x_l \approx \sum_{i=1}^{K} T_{i,A_{i,l}} = \sum_{i=1}^{K} q_i^T C_{i,A_{i,l}}$$
In the end, certain number of vectors with highest approximations is reported.
For each vector $x_l$, $l = 1,2,...,n$, squared error arising from clustering can be expressed this way:
$$ \sum_{i=1}^{K}  [q_{i}^{T} (C_{i, A_{i,l}} - p_{l,i})]^2$$

\begin{algorithm}
	\caption{Quantization-based clustering}
	\begin{algorithmic}
		\For{$i = 0, 1, ..., K$}
			\State $P_i = \emptyset$
		\EndFor
		\For{$l = 0, 1, ..., n$}
			\State $x_l' \gets$ Permute($x_l$)
			\State $part[0], part[1], ..., part[K] \gets$ MakeParts($x_l'$)
			\For{$i = 0, 1, ..., K$}
				\State $P_i = P_i \cup part[i]$
			\EndFor
		\EndFor
		\For{$i = 0, 1, ..., K$}
			\State $C_i = \{C_{i,1}, C_{i,2}, ..., C_{i,k}\}, A_i = \{A_{i,1}, A_{i,2}, ..., A_{i,n}\} \gets$ Kmeans($P_i, K$)
		\EndFor
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Quantization-based querying}
	\begin{algorithmic}
		\State $q' \gets$ Permute($q$)
		\State $q_0, q_1, ..., q_K \gets$ MakeParts($q'$)
		\For{$i = 0, 1, ..., K$}
			\For{$j = 0, 1, ..., k$}
				\State $T_{i,j} = q_i^T C_{i,j}$
			\EndFor
		\EndFor
		\For{$l = 0, 1, ..., n$}
			\State Result[$l$] $\gets 0$
			\For{$i = 0, 1, ..., K$}
				\State Result[$l$] $\gets$ Result[$l$]$+ T_{i,A_{i,l}}$
			\EndFor
		\EndFor
		\State \Return $\argmax_{l}$ Result[$l$]
	\end{algorithmic}
\end{algorithm}

The runtime of this query algorithm is a sum of time taken for building $T$ table and then
quickly calculating the inner product of each vector with query using $T$ as lookup table.
The former does $O(K \cdot k)$ inner products, but each of those is only $O(\frac{d}{K})$
elements long (where $d$ is number of dimensions of original vectors). 
Multiplying these, we get $O(k \cdot d)$ floating-point multiplications.
The second loop takes $O(n \cdot K)$ floating-point \textit{additions}. Additions are often
cheaper than multiplications, but that does not change the asymptotics.

Total runtime is thus $O(K \cdot (k + n))$, but since the number of centroids is bounded by
the number of data vectors, the second term dominates and the expression can be simplified to
$O(K \cdot n)$. This is still linear in $n$ like full search, but has a smaller multiplicative
term, leading to theoretical $O(\frac{d}{K})$ speedup (with possible extra speedup from changing
multiplication to addition).
