\section{Analysis of results}

The amount of data collected is quite large, so directly plotting the results would be
unreadable. Instead, we processed the data selecting, for each algorithm and dataset,
the nondominated parameter choices --- that is those, for which there is no other
parameter choice that gives results that are better in both query time and precision.

Algorithms were always called with request for top 100 vectors, but we recorded
precision at a couple of points: 1, 5, 25 and 100. It turns out the precision vs. time
plots are very similar for each of those values --- to save space, we only show
graphs for precision at 100.

\begin{figure}
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/Amazon-3M-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/sift-p100.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/WikiLSHTC-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/siftsmall-p100.eps}
	}
\caption{Pareto frontiers for each algorithm.}
\end{figure}

The graphs show some similarity --- in all cases $K$-Means and IVF algorithms were much better
than the alternatives. ALSH algorithm gave very poor results on Amazon-3M and Wiki-LSHTC
datasets --- apparently sift vectors have some regularity that ALSH could exploit better.
Still, the precision did not exceed approximately $0.60$ for reasonable speedups even on
this dataset. Finally, the quantization-based algorithm, although it was able to give
reasonable precision, it was so slow, that the only dataset it did not time 
out\footnote{Timeout was set to twice the time of brute force search.}
was SIFTsmall dataset. It contained only ten thousand vectors though and was included
just for completion.

IVF and $K$-Means curves follow each other quite closely --- it could be expected though,
since $K$-Means algorithm is generalization of IVF. In almost all places $K$-Means are even
better than IVF --- again, this can be explained by having more parameters to be tested.
To check whether this is caused by possibly faster implementation or the introduction of
hierarchy, we split $K$-Means curve into three lines, each corresponding to a single
number of layers (for example, KMeans-2 means $K$-Means with two layers).

\begin{figure}
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/Amazon-3M-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/sift-p100.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/WikiLSHTC-p100.eps}
	}
	\caption{Pareto frontiers for $K$-Means (split) and IVF.}
\end{figure}

From these graphs, we can see that in general IVF is approximately $30\% - 50\%$ faster
(on horizontal axis) than $K$-Means when using just a single layer. Both algorithms are
equivalent in this case, so this means Faiss library is more optimized for this parameter 
choice\footnote{
Checking its source we can see it performs inner products of each query with all centroids
(from the first and only layer) all at once, before continuing to answer each query 
using its candidates. This allows one to use heavily optimized matrix-matrix multiplication
routines for the first phase, with additional memory locality advantages. 
In principle, this optimization
could be implemented for hierarchical $K$-Means as well (for the first layer only).
}.
In contrast, $K$-Means are much better when allowed to use two or three layers. Addition
of hierarchy speeds calculation to the level of IVF --- for some parameters even surpassing
it, especially for Amazon-3M dataset.

The difference between two-layer $K$-Means and three-layer ones is not as amplified. Since
training time (not pictured here) usually rises quite significantly with number of layers,
the minor additional speedup may not be worth it in practice. Four and more layers
would probably not have noticable impact either.

It is also notable that two- and three-layer $K$-Means curves continue much further left
than IVF. Of course, such quick results come with significant impact on accuracy, but
IVF cannot even reach this speed. In a sense, its accuracy is zero there.
A possible explanation for this cutoff comes from its complexity analysis as described
in introduction to $K$-Means algorithm chapter. No matter what precision is, the minimal
complexity of flat $K$-Means (equivalent to IVF) is $O(\sqrt{N})$. Two- or three-layer
$K$-Means bring this limit down to $O(\sqrt[3]{N})$, or $O(\sqrt[4]{N})$
respectively\footnote{Admittedly with a small multiplicative slowdown.}.

It is interesting to find out which parameters correspond to particular level
of precision or speedup. For that, we plotted precision vs. time for different
values of $n_{train}$ parameter (which determines shape of $K$-Means tree as
described in chapter 2.1 --- generally, higher values correspond to wider
trees).


\begin{figure}
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/time-prec-WikiLSHTC-1.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/time-prec-WikiLSHTC-2.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/time-prec-WikiLSHTC-3.eps}
	}
	\caption{Precision vs. time for various values of $n_{train}$ parameter.}
\end{figure}

The general trend is that the curves corresponding to lower $n_{train}$ perform
better than other at high-speed, low-precision parts of the chart. In other
words, if high precision is needed, high $n_{train}$ is a good choice (and vice
versa).

This result is consistent with our theoretical caclulations in chapter 2.1, which
were performed for the case of $n_{train} \approx n_{query}$ (and called simply $n$).
In that special case, we would expect low $n$ to cause approximation to be very
quick (as very small number of subtrees are considered), but rough. We can
check if this assumption holds by graphing $n_{query}$ vs. time and comparing
it with previous graphs.

\begin{figure}
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/test-time-WikiLSHTC-1.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/test-time-WikiLSHTC-2.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/test-time-WikiLSHTC-3.eps}
	}
	\caption{Time vs. $n_{query}$ for various values of $n_{train}$ parameter.}
\end{figure}

The charts are quite noisy, since there is some random factor involved. Still,
let us consider two-layer case. On the previous graphs, we could see 
$n_{train} = 5$ was dominating for times in range approximately 0.02 to 
0.04 (fraction of full search time). Now, looking at time vs. $n_{query}$ graph,
we get that range of $n_{query}$ corresponding to that time range is about
3 to 10. Similarly, for $n_{train} = 30$, time range is about 0.04 to 0.12
(though the upper limit is hard to tell due to noise), with corresponding
$n_{query}$ range of about 10 to 60. In both cases, the $n_{query}$ range we
get is centered near $n_{train}$, meaning that for optimal 
$(n_{train}, n_{query})$ choices, both parameters are approximately 
equal\footnote{
Note that the graphs only show a few chosen values of $n_{train}$. Plotting
more values would narrow their non-domination range, strengthening the 
stated assumption. We did not do this due to graph readability issues
(lines being hard to tell apart).
}.
