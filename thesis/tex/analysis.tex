\section{Analysis of results}

The amount of data collected is quite large, so directly plotting the results would be
unreadable. Instead, we processed the data selecting, for each algorithm and dataset,
the nondominated parameter choices --- that is those, for which there is no other
parameter choice that gives results that are better in both query time and precision.

Algorithms were always called with request for top 100 vectors, but we recorded
precision at a couple of points: 1, 5, 25 and 100. It turns out the precision vs. time
plots are very similar for each of those values --- to save space, we only show
graphs for precision at 100.

\begin{figure}
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/Amazon-3M-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/sift-p100.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/WikiLSHTC-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated/siftsmall-p100.eps}
	}
\caption{Pareto frontiers for each algorithm.}
\end{figure}

The graphs show some similarity --- in all cases Hierarchical $K$-Means and IVF algorithms were much better
than the alternatives. ALSH algorithm gave very poor results on Amazon-3M and Wiki-LSHTC
datasets --- apparently the SIFT vectors have some regularity that ALSH could exploit better.
Still, the precision did not exceed approximately $0.60$ for reasonable speedups even on
this dataset. Finally, the quantization-based algorithm, although it was able to give
reasonable precision, it was so slow, that the only dataset it did not time 
out\footnote{Timeout was set to twice the time of the brute force search.}
was SIFTsmall dataset. It contained only ten thousand vectors though and was included
just for completion.

IVF and Hierarchical $K$-Means curves follow each other quite closely --- it could be expected though,
since Hierarchical $K$-Means algorithm is generalization of IVF. In almost all places Hierarchical $K$-Means are even
better than IVF --- again, this can be explained by having more parameters to be tested.
To check whether this is caused by possibly faster implementation or the introduction of
hierarchy, we split the Hierarchical $K$-Means curve into three lines, each corresponding to a single
number of layers (for example, KMeans-2 means Hierarchical $K$-Means with two layers).

\begin{figure}
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/Amazon-3M-p100.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/sift-p100.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/nondominated-km/WikiLSHTC-p100.eps}
	}
	\caption{Pareto frontiers for $K$-Means (split) and IVF.}
\end{figure}

From these graphs, we can see that in general IVF is approximately $30\% - 50\%$ faster
(on horizontal axis) than Hierarchical $K$-Means when using just a single layer. Both algorithms are
equivalent in this case, so this means the Faiss library is optimized for the flat clustering\footnote{
Checking the source of Faiss we can see it performs inner products of each query with all centroids
(from the first and only layer) all at once, before continuing to answer each query 
using its candidates. This allows one to use heavily optimized matrix-matrix multiplication
routines for the first phase, with additional memory locality advantages. 
In principle, this optimization
could also be implemented for Hierarchical $K$-Means (for the first layer only).
}.
In contrast, Hierarchical $K$-Means are much better (achieve same level of precision faster)
when allowed to use two or three layers. The use of hierarchy speeds the calculations to the level 
of IVF --- for some parameters even surpassing
it, especially for Amazon-3M dataset.

The difference between two-layer and three-layer Hierarchical $K$-Means is not as amplified. Since
training time (not pictured here) usually rises quite significantly with the number of layers,
the minor additional speedup may not be worth it in practice. Four and more layers
would probably not have noticeable impact either.

It is also notable that two- and three-layer $K$-Means are able to achieve much higher speed up
than IVF, which manifests on graphs as $K$-Means curves starting earlier than $IVF$ curves. 
Of course, such speed up comes with significant impact on accuracy, but
IVF cannot even reach this speed. In a sense, its accuracy is zero there.
A possible explanation for this cutoff comes from its complexity analysis as described
in the introduction to the Hierarchical $K$-Means algorithm chapter. No matter what precision is, the minimal
complexity of flat $K$-Means (equivalent to IVF) is $O(\sqrt{N})$. Two- or three-layer
$K$-Means bring this limit down to $O(\sqrt[3]{N})$, or $O(\sqrt[4]{N})$, 
respectively.\footnote{Admittedly with a small multiplicative slowdown.}

It is interesting to find out which parameters correspond to particular levels
of precision or speedup. For that, we plotted precision vs. time for different
values of $n_{train}$ parameter (which determines shape of the $K$-Means tree as
described in Chapter 2.1 --- generally, higher values correspond to wider
trees).


\begin{figure}
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/time-prec-WikiLSHTC-1.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/time-prec-WikiLSHTC-2.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/time-prec-WikiLSHTC-3.eps}
	}
	\caption{Precision vs. time for various values of the $n_{train}$ parameter.}
\end{figure}

The general trend is that the curves corresponding to lower $n_{train}$ perform
better than other at high-speed, low-precision parts of the chart. In other
words, if high precision is needed, high $n_{train}$ is a good choice (and vice
versa). Note that Auvolat version of this algorithm is equivalent to our
generalization with $n_{train}$ fixed to $1$. For higher precision 
scenarios, it usually performs worse than higher $n_{train}$ runs.

This result is consistent with our theoretical calculations in Chapter 2.1, which
were performed for the case of $n_{train} \approx n_{query}$ (and called simply $n$).
In that special case, we would expect low $n$ to cause approximation to be very
quick (as a very small number of subtrees is considered), but rough. We can
check if this assumption holds by plotting $n_{query}$ vs. time and comparing
it with previous graphs.

\begin{figure}
	\centering
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/test-time-WikiLSHTC-1.eps}
	}
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/test-time-WikiLSHTC-2.eps}
	}
	\\
	\subfloat{
		\includegraphics[width=.48\textwidth]{../graphs/kmeans-specific/test-time-WikiLSHTC-3.eps}
	}
	\caption{Time vs. $n_{query}$ for various values of $n_{train}$ parameter.}
\end{figure}

The charts are quite noisy, since there is some random factor involved. Still,
let us consider the two-layer case. On the previous graphs, we could see 
$n_{train} = 5$ was dominating for times in the range approximately from 0.02 to 
0.04 (fraction of the full search time). Now, looking at the time vs. $n_{query}$ graph,
we get that range of $n_{query}$ corresponding to that time range is about
3 to 10. Similarly, for $n_{train} = 30$, the time range is about 0.04 to 0.12
(though the upper limit is hard to tell due to noise), with corresponding
$n_{query}$ range of about 10 to 60. In both cases, the $n_{query}$ range we
get is centered near $n_{train}$, meaning that for optimal 
$(n_{train}, n_{query})$ choices, both parameters are approximately 
equal.\footnote{
Note that the graphs only show a few chosen values of $n_{train}$. Plotting
more values would narrow their non-domination range, strengthening the 
stated assumption. We did not do this due to graph readability issues
(lines being hard to tell apart).
}
