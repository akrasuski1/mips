In MIPS we are usually given a large set $S$ of data vectors (also called a database) and a set of queries
(which are vectors as well). Single query is denoted by $q$.
These vectors are often high dimensional --- the number of dimensions can be in range of hundreds or thousands.
Every vector from database and every query represents an object with a number of features equal to the number of dimensions.
The essence of MIPS is for each query to retrieve a fixed number (denoted by $k$, typically ranging
from $5$ to $100$)
of objects from database that are most similar to query.
The retrieval process is also called querying or prediction.

All vectors should be prepared in such a way that the higher inner product between them is, 
the more similar objects they represent are
 --- so inner product is the similarity metric of data vectors. 
Having said that, the goal is to find

$$ Q = \text{argmax}_{x \in S}^{(k)} \ q^T x $$

which means set of $k$ data vectors with highest inner product with query $q$.

The MIPS problem is closely related to Maximum Cosine
Similarity Search (MCSS) and Nearest Neighbour Search (NNS).
The goal in MIPS is to find a vector maximizing the inner product with the query. In turn, 
in MCSS we maximize the cosine similarity between the vector and the query (or equivalently,
minimize the angle between the vector and the query), and in NNS we minimize the distance between the vector and the query. All three problems are
equivalent if all vectors are of unit length. In general, this is not a case.
There exist, however, a couple of known ways
of transforming data and query vectors in order to transform the MIPS problem to MCSS or NNS in the transformed space. We compare two such 
transformations, one introduced by Shrivastava and Li \cite{alsh}, and the second by
Neyshabur and Srebro \cite{neyshabur}.
We will call them SL and NS transformations respectively.

The first transformation has two parameters: $m$ and $U$, such that $0 < U < 1$. All queries
are first normalized, and all vectors divided by the same number such that
their norm is smaller than $U$. Neither of these changes affect
$ \argmin_{i}\ x_i \cdot q\,.$
Then, the following transformations are applied to data
and query vectors:\footnote{In other papers, the $\frac{1}{2}$ fractions are
instead moved to $x'$, for example $\frac{1}{2} - ||x||^2$, leaving extra
query components equal to 0. This does not change the analysis though.}
\begin{eqnarray*}
x' & = & [x, ||x||^2, ||x||^4, \cdots, ||x||^{2^m}] \,, \\
q' & = & [q, \frac{1}{2}, \frac{1}{2}, \dots, \frac{1}{2}] \,. 
\end{eqnarray*}
Both transformations extend vectors by $m$ positions. It can be shown that
the solution to NNS problem in this space is the same as solution to the original
MIPS problem, as long as $m$ is sufficiently large:
\begin{eqnarray*}
||x' - q'||^2 & = & ||x - q||^2 +
\left (||x||^2 - \frac{1}{2} \right )^2 + \left (||x||^4 - \frac{1}{2} \right )^2 + \dots + 
\left (||x||^{2^m} - \frac{1}{2} \right )^2 \\
& = & ||x||^2 + ||q||^2 - 2 q \cdot x + ||x||^{2^{m+1}} + \frac{m}{4} - ||x||^2 \\
& = & 1 + \frac{m}{4} - 2 q \cdot x + ||x||^{2^{m+1}} \,.
\end{eqnarray*}

In the above $1 + \frac{m}{4} $ is a constant, so it does not change the NNS solution. Similarly  the last term, 
$||x||^{2^{m+1}}$, quickly vanishes as $m \to \infty$, so it can be skipped
as an approximation. Shrivastava and Li suggest using $m \ge 3$ for good results.
This leaves just the $ - 2 q \cdot x $ term, which is, up to a multiplicative
constant, the metric optimized in MIPS.
Thus:
$$
\argmax_{i}\ x_i \cdot q \simeq \argmin_{i}\ ||x' - q'||
$$

Neyshabur and Srebro simplify the augmentation significantly. Here, queries are also
normalized, and vectors scaled such that they are within unit sphere. The
transformation is then:
\begin{eqnarray*}
x' & = & [x, \sqrt{1 - ||x||^2}] \\
q' & = &  [q, 0] \,.
\end{eqnarray*}
The inner product of modified vectors is $x' \cdot q' = x \cdot q$, i.e., equal
to the original inner product. MIPS on transformed vectors has thus the same
solution as MIPS on original ones. There is however a useful property of this
augmentation: it makes all vectors' norms equal to one: 
\begin{eqnarray*}
||q'|| & = & ||q|| = 1 \\
||x'||^2 & = & ||x||^2 + \sqrt{1 - ||x||^2}^2 = ||x||^2 + 1 - ||x||^2 = 1 \,. 
\end{eqnarray*}
That means new MIPS problem is equivalent to MCSS and NNS in the transformed
space.

Hierarchical $K$-means and ALSH approximations for MIPS problem may use one
of these two augmentations to improve the results. In our implementation,
algorithms are parametrized by the type of the transformation used: SL, NS,
or no transformation as a baseline.
