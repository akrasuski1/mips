\section{Experimental Design}

    We've conducted extensive experiments to evaluate performance of our algorithms. We've used
    four different datasets, described in detail in section~\ref{sec:datasets}, and compared
    our implementation with a highly-optimized, open source \texttt{FAISS} library released by
    \texttt{Facebook AI Research} group.

    \subsection{Evaluation metrics}

        We have compared the algorithms according to two metrics -- \texttt{test-time}
        and \texttt{precision@}$k$.

        \texttt{Test-time} is defined simply as a number of seconds it takes the index to return
        top-$100$ (approximate) nearest neighbours for each vector in a set of queries provided by the user.
        We assume that queries are available as a contiguous chunk of data kept in-memory.

        \texttt{Precision-@}$k$ can be intuitively described as a
        \textit{"number of relevant items in the list of $k$ returned items, divided by $k$".}

        More formally, given a vector of scores assigned to each item $\hat{\mathbf y} \in {\mathcal{R}}^{L}$, and a
        binary vector indicating which items are relevant $\mathbf y \in \left\lbrace 0, 1 \right\rbrace^L$
        we can write

        \begin{equation}
            \text{P}@k := \frac{1}{k} \sum_{l\in \text{rank}_k (\hat{\mathbf y})} \mathbf y_l
        \end{equation}

        where $\text{rank}_k(\mathbf y)$ returns the $k$ largest indices of $y$ ranked in descending order.

        We have decided not to focus on the training time of the evaluated indices. This
        descision is motivated by the fact, that in most practical use-cases, where
        approximate inference methods will be used, the training time is a one-time cost that
        often can be amortized (e.g by using more compute-power).
        Inference in these cases, however, is the time-sensitive component
        that cannot be easily sped-up (e.g the inference happens on a mobile device, or a small web-server).

    \subsection{Datasets}\label{sec:datasets}

        We have used four datasets to evaluate performance of our algorithms.

        Two of them, \texttt{SIFT} and \texttt{SIFTsmall} were introduced in \texttt{TODO: cite} and their
        main purpose was to allow for evaluation of (approximate) \texttt{knn} algorithms.

        For these datasets a set of $n$ "databse" vectors is given with dimensionality
        $d=128$, as well as a set of "query" vectors $\in \mathcal{R}^d$
        and the "groundtruth" indices. For every "query" vector there are $100$ "groundtruth" indices
        that point to the closest (according to $L_2$ distance) vectors in the database.

        First of all, for every query, we consider only the first vector in the groundtruth as relevant.
        Secondly, we recompute the groundtruth indices so that they're sorted accroding to inner-product
        metric.

        For the two remaining datasets, we have used \texttt{Wiki-LSHTC} \texttt{TODO: cite} and \texttt{Amazon-3m}
        \texttt{TODO: cite} datasets, publicly available at \texttt{TODO: cite}.

        Given the scope of our thesis, they are certainly the most interesting from the machine learning perspective,
        but at the same time pose several challanges.

        The reason these datasets are more meaningfull than \texttt{SIFT} is because they allow us to test our
        algorithms in a real-world setting, where there is a linear model trained to perfrom a multilabel
        classification, for which inference procedure has to be sped up.

        This means that we need to first build a decent classifier, then plug in our approximate algorithms,
        and finally benchmark the whole system.

        We note that there are some constraints when it comes to the choice of the classifier. The first condition
        is that the model should be bottlenecked by a large number of inner products that have to be performed during
        inference step. The second is that these inner products should only deal with dense vectors, since both
        out algorithms and reference \texttt{FAISS} library can only cope with such representations.

        We have experimented with several models, including deep neural networks, but finally decided to use
        \texttt{TODO: cite} \texttt{FastText} classifier. It's a very simple, yet powerfull architecture, that
        can be efficiently trained on very large datasets. It's speed comes from the fact that it uses an efficient,
        asynchronous, 1-bit stochastic gradient descent, and doesn't compute full softmax over the output space,
        but approximates it with a negative sampling \texttt{TODO: cite}.

        \texttt{FastText} takes an input example in a form of a sparse vector (in our case, each index in that vector
        represents a word in the input sentence), and computes its hidden, dense representation. Then, this
        representation is multiplied with a weight vector for each output class, giving us final predictions.
        This last step is exactly the \texttt{MIPS} problem we are discussing in this work.

        Naturally, we wanted to use our approximate algorithms to avoid computing dot product between a hidden
        representation and a weight vector for each class. To be sure that our timings are as accurate as possible,
        we decided to not plug our methods directly into the model. Instead, we first computed the hidden representation
        for each example in the dataset, and saved those to disk. For each example we also saved a set of
        true labels associated with this example. These were our "relevant" classes for each item.
        Finally, we extracted the weight vectors associated with each class, and saved those as well. Givent this setup,
        it was now straightforwad to perfom experiments -- we just treated the weight vectors as our database vectors,
        and hidden representations as query vectors.

        For completeness, below we summarize the descriptive statistics of the used datasets.

        WRITEME(elan): tabel with dataset stats


    \subsection{Baseline (IVF)}

        We have compared our implementations with a highly-optimized \texttt{FAISS} (TODO) library.
        Specifically, we have decided to use the \texttt{Inverted-File-Index} algorithm it provides.

        This choice was motivated by two things.
        Firstly, the authors of the library have empirically determined that this algorithm performs
        really well in settings similar to ours -- where we are not memory constrained, and whole dataset
        can fit in memory.
        Secondly, this algorithm is a special case of our \texttt{Hierarchichal K-means}, specifically
        it is equivalent to \texttt{hierarchical k-means} with number of levels set to $!$.

    \subsection{Parameter grid}

        After an initial round of manual tests we have gained a pretty good understanding of
        each algorithm, and were able to design a grid of sensible parameters for each of them.

        For \texttt{Hierarchical K-means} the core parameters are of course the depth of the hierarchy
        (we have experimented with up to depth of $4$), number of centroids on each level,
        and the number of nodes that are expanded during search procedure.

        For \texttt{Asymmetric LSH} the most important parameters were number of hash tables ($L$),
        and number of hash functions $K$. The rest of the parameters was mostly kept fixed, following
        recommendations of the original ALSH paper.

        For \texttt{Quantization} we have varied the number of subspaces as well as number of centroids
        in each subspace.

        For a more detailed explanation of the parameters mentioned above, please see the sections
        dedicated to each of the algorithms.
        We provide the exact parameters used in the Appendix (TODO).

    \subsection{Amazon}

        All of our experiments were conducted using \texttt{Amazon Web Services} infrastructure.
        More concretely we have used compute-optimized \texttt{c4.4xlarge} instances. Each such
        instance provides $16$ virtual cores and $30$ gigabytes of \texttt{RAM}, and is powered
        by an \texttt{Intel Xeon E5-2666 v3 (Haswell)} processor optimized specifically for AWS.

        We have provisioned around $40$ machines, which allowed us to evaluate our parameter grid
        in just few days.
        At the same time, however, it was unfesable to manage this number of machines manually.
        To overcome this problem, we have prepared a simple scheduler,
        that would take a grid of parameters and list of machines as inputs, and schedule
        jobs to be executed on those machines.
        An \texttt{ssh} tunnel was then established with each machine so that scheduler
        was able to request a job execution, and a machine was able report the evaluation results.
