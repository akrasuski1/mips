\section{Design of the experiments}\label{sec:experimentalDesign}

    We have conducted extensive experiments to evaluate performance of our algorithms. We have used
    four different datasets, described in detail in section~\ref{subsec:datasets}, and compared
    our implementation with a highly-optimized, open source Faiss library~\cite{faiss} released by
    Facebook AI Research group.

    \subsection{Evaluation metrics}\label{subsec:evaluationMetrics}

        We have compared the algorithms according to two metrics -- \texttt{test-time}
        and \texttt{precision@}$k$.

        \texttt{Test-time} is defined simply as a number of seconds it takes the index to return
        top-$100$ (approximate) nearest neighbours for each vector in a set of queries provided by the user.
        We assume that queries are available as a contiguous chunk of data kept in-memory.

        \texttt{Precision@}$k$ (P@$k$) can be intuitively described as a
        ``number of relevant items in the list of $k$ returned items, divided by $k$''.

        More formally, given a vector $\hat{\mathbf y} \in {\mathcal{R}}^{L}$ of scores assigned to each item, and a
        binary vector $\mathbf y \in \left\lbrace 0, 1 \right\rbrace^L$ indicating which items are relevant
        we can write
%
        \begin{equation*}
            \text{P}@k := \frac{1}{k} \sum_{l\in \text{rank}_k (\hat{\mathbf y})} y_l
        \end{equation*}
%
        where $\text{rank}_k(\hat {\mathbf y})$ returns indices of $k$ labels with the highest scores in $\hat{\mathbf y}$.

        We have decided not to focus on the training time of the evaluated indices. This
        decision is motivated by the fact, that in most practical use-cases, where
        approximate inference methods will be used, the training time is a one-time cost that
        often can be amortized (e.g by using more compute-power).
        Inference in these cases, however, is the time-sensitive component
        that cannot be easily sped-up (e.g the inference happens in real-time on a mobile device, or a small web-server).

    \subsection{Datasets}\label{subsec:datasets}

        We have used four datasets to evaluate performance of our algorithms.

        Two of them, SIFT and SIFTsmall were introduced in~\cite{jegou2011product} and their
        main purpose was to allow for evaluation of (approximate) $k$-nearest neighbors algorithms. They are freely
        available for download at~\cite{sift}.

        For these datasets a set of $n$ ``database'' vectors is given with dimensionality
        $d=128$, as well as a set of ``query'' vectors $\in \mathcal{R}^d$
        and the ``groundtruth'' indices. For every ``query'' vector there are $100$ ``groundtruth'' indices
        that point to the closest (according to $L_2$ distance) vectors in the database.

        First of all, for every query, we consider only the first vector in the groundtruth as relevant.
        Secondly, we recompute the groundtruth indices so that they're sorted accroding to the inner-product.

        For the two remaining datasets, we have used Wiki-LSHTC~\cite{lshtc} and
        Amazon-3M~\cite{a3m} datasets, publicly available at~\cite{exrepo}.
        Given the scope of our thesis, they are certainly the most interesting from the machine learning perspective,
        but at the same time pose several challanges.

        The reason these datasets are more meaningful than SIFT is because they allow us to test our
        algorithms in a real-world setting, where there is a linear model trained to perform multilabel
        classification, for which the inference procedure has to be sped up.
        This means that we need to first build a decent classifier, then plug in our approximate algorithms,
        and finally benchmark the whole system.

        We note that there are some constraints when it comes to the choice of the classifier. The first condition
        is that the model should be bottlenecked by a large number of inner products that have to be performed during
        inference step. The second is that these inner products should only deal with dense vectors,
        since both the reference Faiss library and our algorithms can only cope with such representation.

        We have experimented with several models, including deep neural networks, but finally decided to
        use the fastText~\cite{fasttext} classifier. It is a very simple, yet powerfull architecture, that
        can be efficiently trained on very large datasets. Its speed comes from the fact that it uses an efficient,
        asynchronous, stochastic gradient descent, and doesn't compute full softmax over the output space,
        but approximates it with a negative sampling~\cite{w2v}.

        Here, by stochastic gradient descent we mean a training procedure, in which weights of the model are updated
        after observing each sample in the dataset. This is in contrast with popular in deep learning
        mini-batch gradient descent, where a gradient is averaged over a number of samples (a mini-batch),
        and only then the update step is performed.

        We write that we use asynchronous gradient decent, since during trainin a number of threads is spawned. Each
        of those threads computes its own gradients, but updates a shared weight matrix. This is done without any
        form of synchronization.

        Finally, negative sampling is a technique used for training models with large numbers of output classess, where
        for each example and a set of positive labels, only a small set of negative labels is sampled from the dataset.
        This is in contrast with, for example, softmax, which uses positive and all negative labels to compute the loss.

        As for the model itself, fastText takes an input example in a form of a sparse vector
        (in our case, each index in that vector represents a word in the input sentence), and computes its hidden,
        dense representation. Then, this representation is multiplied with a weight vector for each output class,
        giving the final prediction. This last step is exactly the MIPS problem we are discussing in this work.

        Naturally, we wanted to use our approximate algorithms to avoid computing dot product between a hidden
        representation and a weight vector for each class. To be sure that our timings are as accurate as possible,
        we decided to not plug our methods directly into the model. Instead, we first computed the hidden representation
        for each example in the dataset, and saved those to disk. For each example we also saved a set of
        true labels associated with this example. These were our ``relevant'' classes for each item.
        Finally, we extracted the weight vectors associated with each class, and saved those as well. Given this setup,
        it was now straightforward to perfom experiments --- we just treated the weight vectors as our database vectors,
        and hidden representations as query vectors.

        For completeness, we summarize the descriptive statistics of the used datasets in Table~\ref{tab:dsz}.

        \begin{table}
          \caption{Data statistics: $d$ is the dimensionality of the feature vectors (for Wiki-LSHTC and Amazon-3M this
          is the dimensionality of the fastText hidden representation), $L$ is the number
          of database vectors (for Wiki-LSHTC and Amazon-3M this is the number of labels), and $n$ is the number of query vectors.}
          \label{tab:dsz}
          \centering
          {
          \begin{tabular}{l|c|c|c|}
            Dataset & $d$ &$L$& $n$ \\
            \hline
            SIFTsmall   & $128$ & $10000$   & $100$    \\
            SIFT        & $128$ & $1000000$ & $10000$  \\
            Wiki-LSHTC  & $256$ & $325056$  & $587084$ \\
            Amazon-3M   & $256$ & $2812281$ & $742507$ \\
          \end{tabular}
          }
        \end{table}

    \subsection{Baseline (IVF)}\label{subsec:baselineivf}

        We have compared our implementations with a highly-optimized Faiss~\cite{faiss} library.
        Specifically, we have decided to use the Inverted File Index algorithm it provides.

        This choice was motivated by two factors.
        Firstly, the authors of the library have empirically determined that this algorithm performs
        really well in settings similar to ours --- where we are not memory constrained, and whole dataset
        can fit in memory.
        Secondly, this algorithm is a special case of our hierarchical $K$-means, specifically
        it is equivalent to hierarchical $K$-means with the number of layers set to $1$.

    \subsection{Parameter grid}\label{subsec:parameterGrid}

        After an initial round of manual tests we have gained a pretty good understanding of
        each algorithm, and were able to design a grid of sensible parameters for each of them.

        For hierarchical $K$-means the core parameters are of course the depth of the hierarchy
        (we have experimented with depth up to $4$), the number of centroids on each level,
        and the number of nodes that are expanded during search procedure.

        For ALSH the most important parameters were the number of hash tables ($L$),
        and the number of hash functions ($K$). The rest of the parameters was mostly kept fixed, following 
        recommendations of the original ALSH paper.

        For the quantization-based algorithm we have varied the number of subspaces as well as the number of centroids
        in each subspace.

        For a more detailed explanation of the parameters mentioned above, please see the sections
        dedicated to each of the algorithms.

    \subsection{Amazon Web Services}

        All of our experiments were conducted using the Amazon Web Services infrastructure.
        More concretely we have used compute-optimized \texttt{c4.4xlarge} instances. Each such
        instance provides $16$ virtual cores and $30$ gigabytes of RAM, and is powered
        by an Intel Xeon E5-2666 v3 (Haswell) processor optimized specifically for AWS.

        We have provisioned around $40$ machines, which allowed us to evaluate our parameter grid
        in just few days.
        At the same time, however, it was unfeasible to manage this number of machines manually.
        To overcome this problem, we have prepared a simple scheduler,
        that would take a grid of parameters and list of machines as inputs, and schedule
        jobs to be executed on those machines.
        An SSH tunnel was then established with each machine so that scheduler
        was able to request a job execution, and a machine was able to report the evaluation results.
