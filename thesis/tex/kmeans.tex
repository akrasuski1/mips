
A common approach for speeding up search of vector maximizing certain measure,
such as inner product with a query vector, is preprocessing data vectors to
quantize them to a smaller set of vectors. Quantization in this case means
finding a relatively small set $C$ of quantization vectors and a mapping
from original data vectors to that set. Quantization vectors are usually
centroids (arithmetic averages) of all data vectors mapping to it, and the
mapping is chosen in a way making quantization error (difference between
data vector and its centroid) as small as possible.

This technique assumes that a vector
will have similar value of the target measure --- in our case inner product --- 
as its quantized representation.
This is true for maximum inner product search --- if point $x$ is assigned
to centroid $c$ and the distance between them (quantization error) is $d$,
then error in estimating inner product of query vector $q$ with $x$ by inner
product of query with centroid is smaller than $d$ (for normalized queries):

$$ |q \cdot c - q \cdot x | = | q \cdot ( c - x ) | \le ||q|| \cdot d = d $$

The speedup of this technique comes from the fact that set of quantized vectors
can have smaller cardinality than the set of original vectors. This leads to a 
useful heuristic while searching: first multiply 
query vector with all quantized vectors, select the one maximizing inner
product, and then, for the final answer, consider only vectors having this vector
as its quantization. This algorithm may in general give inexact answers
because of quantization errors --- but it is a good approximation.

A simple, but effective way to quantize vectors is $K$-means clustering.
Formally, the clustering problem is to produce a set of $K$ vectors, known as centroids, and assign
each data vector to one of them in such a way, that minimizes sum of squared
distances of each vector to its centroid. Produced clusters (vectors
assigned to the same centroid) have often relatively small radius (maximum
distance between point in cluster to its centroid). That is a good property 
for our problem, as it means that the inner product with the query will be well approximated
by $q \cdot c$, where $c$ is the centroid vector.

Finding the optimal solution is in general NP-hard, but there are efficient
approximations for it. The most common and simple algorithm consists of two steps performed 
alternately a number of
times. At each iteration, a set of centroids $C$ is maintained (randomly
initialized at the start of the algorithm). The first
step of an iteration is assignment of each data vector $x_i$ to the
centroid $C_{A_i}$ that
is nearest to it. Then, in a second step, each centroid's value is updated
to a new value, equal to arithmetic average of all vectors assigned to
it.\footnote{
In case of MCSS, a modified algorithm called spherical $K$-means can be used.
It is essentially the same, with the only difference being the additional
centroid normalization after each update. This is because MCSS
takes into account only the vector direction, ignoring the magnitude. 
} The procedure is formally described in Algorithm~\ref{alg:k-means-cluster}.

\begin{algorithm}
	\caption{$K$-means algorithm}
	\begin{algorithmic}
		\State $C \gets K \text{ random vectors}$
		\Repeat
			\For{$ i = 0,\dots,N-1 $}
				\State $A_i \gets \argmin_{j}\ || x_i - C_j || $
			\EndFor
			\For{$ j = 0,\dots,K-1 $}
				\State $C_j \gets$ mean$(\{x_i\ |\ A_i = C_j\}) $
			\EndFor
		\Until {$ C \text{ stayed the same in this iteration} $}

		\State \Return {$C,\ A$}
	\end{algorithmic}
\label{alg:k-means-cluster}
\end{algorithm}

In practice, we use a constant number of 25 outermost loop iterations
instead of waiting for full convergence --- further improvements are negligible.

If straightforward implementation is used, each assignment step performs
$O(N \cdot K)$ vector operations (where $N$ is number of data vectors and
$K$ --- number of centroids), and each clustering step --- $O(K^2)$. Since
$K \le N$, the time complexity is determined by the assignment step. For a real
world scenario with $N = 10^6$ and $K = 4000$, there is approximately
$10^6 \cdot 4000 = 4 \cdot 10^{9}$ vector operations to be done per iteration --- since vectors
can have a couple hundred of components, that figure increases to roughly
$10^{12}$ floating point operations. This takes considerable amount of time,
but it is a one-time calculation --- each query can then be answered without
recalculating centroids.

Algorithm~\ref{alg:k-means-query} describes how to answer query using stored centroids.
First, query's inner product with each centroid is calculated, centroid $c$ maximizing it is
found out, and then the query is multiplied by each vector whose quantization is this centroid.
The final solution is the vector 
with the biggest inner product. In essence, there are two exhaustive searches,
the first one over the set of centroids, and the second over the set of vectors in the chosen cluster.

\begin{algorithm}
	\caption{$K$-means query}
	\begin{algorithmic}
		\State $B \gets -\infty$
		\For{$ i < K $}
			\If{$C_i \cdot q > B$}
				\State $B \gets C_i \cdot q$
				\State $c \gets i$
			\EndIf
		\EndFor
		\State $B \gets -\infty$
		\For{$ j \in \{ j\ |\ A_j = c \} $}
			\If {$x_j \cdot q > B$}
				\State $B \gets x_j \cdot q$
				\State $r \gets j$
			\EndIf
		\EndFor
		\State \Return $r$
	\end{algorithmic}
\label{alg:k-means-query}
\end{algorithm}

Time complexity of this algorithm is the sum of complexities of both loops.
First, there are $O(K)$ vector multiplications to determine the best centroid,
followed by $O(BF)$ inner products with its children\footnote{The second loop
of the algorithm would have to perform $O(N)$ iterations if written naively.
This can be prevented by remembering list of children for each centroid,
reducing complexity to stated $O(BF)$.}.
$BF$ in this case denotes
branching factor, defined as number of children of the centroid. This is rarely
constant, but can be approximated by the average branching factor of all centroids,
which is $O(\frac{N}{K})$, since each of $N$ data vectors has to be assigned
to exactly one centroid. The total complexity is thus $O(K + \frac{N}{K})$. Since
$K$ is a parameter of the algorithm, it can be varied to measure efficiency. It
can be expected that good results should be obtained for $K \approx \sqrt{N}$ 
though, as this value minimizes expression $K + \frac{N}{K}$, for optimal
complexity of $O(\sqrt{N} + \frac{N}{\sqrt{N}}) = O(\sqrt{N})$. However, note that other
values of $K$ result in a different level of approximation --- there is
a tradeoff between time and accuracy. 

There is also a second way of improving accuracy at the cost of time --- more
than one centroid's children can be searched. This increases complexity of the
second loop roughly $n$ times (where $n$ is number of selected centroids),
but increases probability that the returned vector is the closest one. A further
analysis of this scenario will be done in following paragraphs, as part
of the description of hierarchical $K$-means, generalization of this algorithm.

%\bigskip

The $O(\sqrt{N})$ time is a good improvement over full search which 
takes $O(N)$ time. It can still be not enough for extremely large datasets
though, especially if a good level of approximation is needed. A hierarchical
$K$-means scheme has been proposed as a further improvement \cite{kmeans}.

In this approach, data vectors are quantized, like previously,
using $K$-means algorithm.
The difference is that the resulting centroids are clustered again, forming
a second layer of centroids. Querying is similar to the non-hierarchical
approach too --- query is first multiplied with all top-layer centroids,
best of them is selected, then its best child and
so on. This approach can be easily generalized to a higher number $L$  of layers.
The pseudocode of clustering with hierarchical $K$ means is given as Algorithm~\ref{alg:h-k-means}.

\begin{algorithm}
	\caption{Hierarchical $K$-means clustering}
	\begin{algorithmic}
		\State Layers$[0]$.vectors $\gets x $
		\For{$ i = 0,\dots,L $}
			\State Layers$[i]$.size $\gets \left \lfloor N^{\frac{L + 1 - i}{L + 1}} \right \rfloor $
			\State $ C,\ A \gets$ Kmeans(Layers$[i - 1]$.vectors, Layers$[i - 1]$.size)
			\State Layers$[i]$.vectors $\gets C$
			\For {$ c = 0,\dots,$Layers$[i]$.size$-1 $}
				\State Layers$[i]$.children$[c] \gets \{ j\ |\ A[j] = c \} $
			\EndFor
		\EndFor
		\State \Return Layers
	\end{algorithmic}
\label{alg:h-k-means}
\end{algorithm}

\begin{algorithm}
	\caption{Hierarchical $K$-means querying}
	\begin{algorithmic}
		\State $ S \gets [0..$Layers$[L].$size$]$
		\For{$ i = L,\dots,0 $}
			\State $ B \gets -\infty $
			\For{$ s \in S $}
				\State $ I \gets q\ \cdot\ $Layers$[i]$.vectors$[s]$
				\If{$ I > B $}
					\State $ B \gets I $
					\State $ r \gets s $
				\EndIf
			\EndFor
			\If{$ i = 0 $}
				\State \Return {$ r $}
			\EndIf
			\State $ S \gets\ $Layers$[i].$children$[r]$
		\EndFor
	\end{algorithmic}
\end{algorithm}

The running time of this algorithm varies with the number of layers. Size (number of
centroids) of layers can be parametrized, but assuming
the ratio of sizes of consecutive layers is $ \sqrt[L+1]{N} $, as a direct generalization
of $ \sqrt{N} $ from non-hierarchic $K$-means,
then each of the $ L + 1 $ searches will have to perform approximately
$ O(\sqrt[L+1]{N}) $ inner products, for a total complexity of
$ O(\sqrt[L+1]{N} \cdot (L + 1)) $. That means, for example, for the same real-world
example as previously ($ N=10^6 $) with two layers only we need approximately
$ \sqrt[2+1]{10^6} \cdot (2 + 1) = 300 $ inner products, as opposed
to at least $ \sqrt{10^6} \cdot 2 = 2000 $ using a single layer like in flat $K$-means.
Potential time gains are even bigger for 3 or 4 centroid layers.\footnote{
There is a theoretical limit for improvement --- at point where increasing
number of layers leads to slower search because of the multiplicative $L+1$ factor.
This limit is in practice large enough that accuracy degradation is too 
significant for this number of layers to be feasible anyway. 
For $N=10^6$, theoretical limit is $15$ layers, corresponding to average 
of $2.5$ children per centroid, i.e. almost binary tree.
} There is a problem however --- there are few top-layer clusters,
meaning they have many last-layer eventual descendants. That causes the
original algorithm assumption of centroids being close to points they represent
to become weaker, leading to gradual decrease in accuracy with the increasing number of layers.


One way to counter it is to, again, consider $n$ best children instead of
just one. This would increase time complexity to 
$ O(\sqrt[L+1]{N} \cdot (L \cdot n + 1)) $, as all layers except of the top one
will have on average $n$ times more candidates to check.

Auvolat suggests in \cite{kmeans} to use $N^{\frac{L+i-1}{L+1}}$ as cluster
sizes. We note however, that when checking more than one centroid at each layer, 
the optimal (in terms
of inner products performed) sizes of clusters change. Our contribution,
analysis of $n>1$ case, follows.

Let $ K_L $ denote
number of centroids in the top layer, $ K_{L-1} $ the one below, up to
$ K_1 $ meaning the number of centroids in the bottom layer, and $ K_0 $, by 
convention, meaning the number of all data vectors (equal $N$). The total number of
products calculated is equal to 
$$ P = K_L + \sum_{i=0}^{L-1} \frac{K_i}{K_{i+1}}  n \,.$$
In the optimal choice of $K_i$ values, partial derivatives of this expression
with respect to $K_i$ should all be equal 0. The equation for the top layer 
derivative is a special case, as it has an extra component before the sum:
$$ 
\frac{\partial P}{\partial K_L} = 
\frac{\partial (K_L + \frac{K_{L-1}}{K_L} n)}{\partial K_L} =
1 -\frac{n \cdot K_{L-1}}{K_L^2} = 
0
$$
$$
1 = \frac{n \cdot K_{L-1}}{K_L^2}
$$
\begin{equation}
\label{eqn:kl-n}
\frac{K_L}{n} = \frac{K_{L-1}}{K_L}
\end{equation}
The remaining derivatives follow another equations:
$$
\frac{\partial P}{\partial K_i} =
\frac{\partial (\frac{K_i}{K_{i+1}} + \frac{K_{i-1}}{K_i}) \cdot n}{\partial K_i} =
\frac{n}{K_{i+1}} - \frac{n \cdot K_{i-1}}{K_i^2} = 0
$$
$$
\frac{n}{K_{i+1}} = \frac{n \cdot K_{i-1}}{K_i^2}
$$
\begin{equation}
\label{eqn:ki-1}
\frac{K_i}{K_{i+1}} = \frac{K_{i-1}}{K_i}
\end{equation}

From equation \ref{eqn:ki-1} we can see that ratios of consecutive cluster sizes are constant --- in other
words, they form a geometric progression. The first term in the sequence,
$K_0$ is known to be equal to number of data vectors, $N$. From equation \ref{eqn:kl-n},
last --- $(L+1)^{th}$ --- term of the progression is known to be $n$. Combining the results,
the common ratio is calculated to be $\sqrt[L+1]{n}$. The general formula
for the number of centroids on the $i^{th}$ of $L$ layers minimizing the total number of
inner products is hence:
$$
K_i = N^{\frac{L+1-i}{L+1}} \cdot n^\frac{i}{L+1}
$$

For $N = 10^6$, $L = 2$ and $n = 100$, this modified equation gives cluster sizes:
\begin{eqnarray*}
K_1 & = & (10^6)^{\frac{2}{3}} \cdot 100^\frac{1}{3} \approx 46416 \\
K_2 & = & (10^6)^{\frac{1}{3}} \cdot 100^\frac{2}{3} \approx 2154 \,.
\end{eqnarray*}
The number of inner products is thus:
$$ 
P = K_2 + \frac{K_1}{K_2} \cdot n + \frac{N}{K_1} \cdot n =
2154 + \frac{46416}{2154} \cdot 100 + \frac{10^6}{46416} \cdot 100 \approx
6463
$$

In the original form unadjusted for the increased number of considered centroids,
the numbers would be:
\begin{eqnarray*}
K_1 & = & (10^6)^{\frac{2}{3}} = 10000 \\
K_2 & = & (10^6)^{\frac{1}{3}} = 100 \\
P & = & K_2 + \frac{K_1}{K_2} \cdot n + \frac{N}{K_1} \cdot n  \\
 & = & 100 + \frac{10000}{100} \cdot 100 + \frac{10^6}{10000} \cdot 100 \\ 
& = &  20100
\end{eqnarray*}

A roughly threefold improvement can be seen by using the theoretical optima.
This does not mean though that the accuracy stays the same --- it is possible
for the change in cluster sizes to create worse quantization. Another
possible effect can be seen when more than one best vector is required ---
in our benchmarks, top-100 candidates are returned. Smaller clusters near
the bottom layer mean that for constant $n$ parameter, smaller number of
data vectors will be searched.
In either case, the centroids can
be trained using optimal values for some value of $n$, but then be queried
with another parameter $n$ value. Careful tuning of these two parameters, 
$n_{train}$ and $n_{query}$, may lead to satisfying accuracy at quicker time.
