Database is loaded to \texttt{FlatMatrix} and vectors are permuted randomly.
Subspaces (sets of vector parts) are implemented as a \texttt{std::vector} of \texttt{FlatMatrix}.
The width of \texttt{FlatMatrix} is determined keeping in mind that original width may not be
divisible by a required number of subspaces --- if it happens, the last subspace has less dimensions.
Data is then accordingly copied to elements of mentioned \texttt{std::vector}.
Subsequently, the same flat $K$-means procedure which forms the base of hierarchical $K$-means
is performed on every subspace. It returns the most important part of the index:
a structure comprising of \texttt{FlatMatrix} to store centroids and a \texttt{std::vector}
containing assignments of points to centroids.

Before searching the index, queries are likewise loaded to \texttt{FlatMatrix}, permuted and divided into subspaces.
The prediction method operates on two vectors created before, having size equal to the number of subspaces:
a vector of sets of query parts and a vector of sets of $K$-means centroids for every subspace.
Next, another \texttt{FlatMatrix} of inner products between query parts and $K$-means centroids
is computed and remembered.
The products of centroids assigned to the vector in each part and query parts are summed and give a vector score.
Such computed scores are saved in a \texttt{std::vector} of pairs --- a vector index and its score.
This vector of pairs is sorted by the score, truncated to demanded size of $k$ best vectors and their indices are returned.
