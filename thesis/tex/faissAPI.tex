
\subsection{Dependencies}

Our library contains a number of dependencies. Most importantly, the core components rely on 
another library for \textit{Approximate Similarity Search}, Faiss \cite{faiss}.
Faiss in turn depends on Basic Linear Algebra Subprograms (BLAS) being available. In our experiments we have used
the \texttt{OpenBLAS} \cite{openblas} implementation, as it is fully open-source and easiest to install. One
could obtain slightly improved performance with proprietary \texttt{MKL} \cite{mkl} implementation, but this would require
users to perform extra installtion steps.

Additionally, the Python parts of our code rely on several components: \texttt{pybind11} and \texttt{swig}
for bindings generation, standard data-science tools like \texttt{pandas} for data analysis, and naturally, on the
libraries that we create wrappers for. For more details we refer the reader to the \hyperref[sec:appendices]{appendix}.

\subsection{Build System}

Given a moderate size of our project, we decided to use \texttt{make} as our building tool. The \texttt{Makefile} we use
depends on variables set in configurable \texttt{makefile.inc}. Details regarding the build process can be found in the appendix.

\subsection{Performance considerations}

Given that performance was a crucial consideration, we decided to implement our algorithms in C++.
Additionally, whenever possible, we reused highly performant components available as a part of the aforementioned Faiss,
for example we heavily reuse their fast inner product routines. 

A couple of provided routines for fast vector operations, such as inner product calculation, can
operate in batch mode to increase speed. Vectors to be used in calculation are then concatenated
to a contiguous array --- this makes sense, as it increases memory locality in comparison
to ``vector of vectors'' way, where pointer indirection would be involved. Using such a data structure
is quite error prone though, as one has to manually calculate offsets into the array. To improve
usability, we abstracted it to a \texttt{FlatMatrix} structure, which provides matrix-like API, while
still storing data in single array. The underlying data storage is also accessible through a method,
so that it can be used for highly optimized Faiss functions.

Querying is carried out in parallel using available number of processor cores.
It is accomplished by an OpenMP directive
before the loop iterating through queries ensuring high scalability in situations
where many queries have to be answered at once.
Querying is easily parallelized because each query can be answered independently from others.
Moreover, data is only read from indexes, so there is no race condition hazard.
Additionally, in ALSH algorithm vector hashing is processed in parallel with respect to hash tables.

\subsection{Wrapping C++ with Python}

We have used \texttt{pybind11}\cite{pybind11} library to create wrappers around our C++ code. We chose \texttt{pybind11}
due to several reasons: it supports a wide range of \texttt{C++} constructs
(including classes, lambdas, and smart pointers), can generate wrappers for multiple python versions and
implementations (including Python 2.7, 3.x, and PyPy (PyPy2.7 >= 5.7)), and works nicely with \texttt{numpy} arrays.
Additionally, one needs to provide only a single \texttt{C++} source file to create these wrappers, which is
a big advantage compared to e.g. \texttt{SWIG}.

\texttt{numpy} is an extremely popular Python library for scientific computing, and is the backbone of all
machine learning libraries in use today. NumPyâ€™s main object is the homogeneous multidimensional array, which
can be used to hold for example weights of a trained classifier. If we wanted to use our C++ indexes on such
objects, it was crucial to be able to pass them without copying the underlying storage.

Luckily it is quite straightforward in \texttt{pybind11} to write wrappers that can accept a \texttt{numpy} array
and extract a raw \texttt{C} pointer to its data. Conversely, it is also straightforward to create
a \texttt{numpy} array from a raw pointer returned by a \texttt{C} function. This array is then able to take the
ownership of this pointer and realease it upon being garbage collected.

In all of our function we release the \texttt{GIL} -- Python's Global Interpreter Lock. This means that while our
\texttt{C++} is executing, Python interpreter is free to run other tasks. Once our \texttt{C++} code returns, the
interpreter lock is re-aquired.

\subsection{Internal API}

We have decided that our algorithms should expose the same API as defined in Faiss.

Any given \texttt{Index} should implement the following methods:
\begin{itemize}

\item A constructor without parameters that initializes parameters to some reproducible default.

\item \texttt{add(int64\_t n, const float* data)} 
should add \texttt{n} vectors present in \texttt{data} to the index database.

\item \texttt{search(idx\_t n, const float* data, idx\_t k, float* distances, idx\_t* labels)} 
should search \texttt{k} neares neighbours 
in its internal database, for each of \texttt{n} queries present in \texttt{data}, writing results to \texttt{distances} and \texttt{labels}.

\item \texttt{reset(int64\_t, const float*)} 
should reset the index to the default state.

\item \texttt{train(int64\_t n, const float* data)} 
may perform additional tuning of the index on \texttt{n} points present in \texttt{data}. This is
optional, and in fact we leave this function empty.

\end{itemize}
