\section{Aim and scope of the thesis}

The aim of this thesis is to describe a library for fast, approximate \textit{maximum inner product search} (MIPS),
which we created as part of our dissertation.

The goal of this work was to provide a set of tools that would help machine learning developers and practitioners create faster,
more scalable machine learning models.
Problems requiring MIPS naturally arise in many machine learning models, for example in
classification problems with large output spaces, when a linear models or deep neural networks are used.

Generally speaking, MIPS is the task involving searching a massive collection of items for
an item most similar to particular one, called query. The items may have many properties (features)
according to which items can be more related to each other or differ from each other.
Every item is mathematically represented by a vector, with all vector's components being item's features.

Unfortunately, when one has to compare the query with hundreds of thousands of vectors,
a linear search time can become prohibitively expensive.
In other words comparing query and every database vector may not be feasible in practical applications.
To counter that, proposals were made in several articles to build a kind of an \textit{index} for speeding
up the searching time.
This index is essentially a specific data structure built by preprocessing database vectors (before
any of the queries are answered --- this is a one-time cost).
The most important feature of the index is short prediction (searching) time, training (index preparation)
can last much longer if needed.
It is not always necessary for the exact result to be returned --- it can often be approximate.
In this thesis we focus on such algorithms --- they trade some prediction precision for speed.

The library we built aims to deliver indexes, based on algorithms proposed in three articles,
designed to efficiently retrieve a set of approximate results.

Additionally, we wanted the algorithms to not only offer high performance, but also to be easy-to-use. We therefore
focused on providing a set of wrappers around a range of machine learning libraries, that allow users to
speed up their models without ever touching any of the algorithms indexes rely on. Consequently,
these wrappers can be used as drop-in replacements for a number of models in Python machine learning ecosystem.
We show some examples of how this library can be used in practice,
as well as detailed experiments measuring the speed and accuracy of our algorithms.

The rest of this thesis is structured as follows.
Second chapter is dedicated to theoretical description of algorithms aiming to solve MIPS.
Third chapter describes implementation details of written software.
Fourth chapter presents experimental results.
Fifth chapter concludes the thesis.
There is also an appendix at the end describing how to run our library.

Marcin Elantkowski wrote wrappers translating C++ indexes to Python, prepared data using his fork of FastText
library, adapted the code so it could be used from Pytorch and ran final tests on Amazon Web Services.

Adam Krasuski refactored and optimized core algorithms C++ code, implemented Faiss interface,
then after final tests, analyzed the benchmarking results.

Agnieszka Lipska implemented first version of quantization algorithm, implemented Faiss interface and
adapted the code to use it from Scikit-learn.

Franciszek Walkowiak implemented naive versions of ALSH and hierarchical \mbox{k-means} and conducted
preliminary tests on a personal computer.
