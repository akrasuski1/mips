\remark{Structure implementing this algorithm? A structure used by the hierarchical $k$-means algorithm contains mostly the results of hierarchical clustering.} contains mostly the hierarchical clustering results.
Since most of the querying time is spent walking through this tree, effort has been made
to optimize it.

Centroids are stored as a \texttt{std::vector} of layer structures. Layer number $0$ corresponds
to original data vectors, $1$ --- bottom clusters, and so on. Each layer structure contains
\texttt{FlatMatrix} of points making this layer, and for each of them, a way of referring to
its children. In first implementation, this was simply a \texttt{std::vector} of indices into
the lower layer. This was however found to have significant impact on memory performance,
since children could have many gaps of arbitrary size in between (for example, centroid
$1$'s children could be points $1$, $8$ and $13$). To improve cache-friendliness, after
calculating centroid positions and children assignments, the children array itself was 
reordered in such a way, that any given centroid's children are contiguous. In the example
given above, that could mean renaming point $8$ to $2$ and $13$ to $3$. This single
optimization was found to speed up the computations approximately twice.

As for querying implementation, the algorithm is mostly a direct translation of the pseudocode
described in the theoretical part of the thesis. The only significant modification is 
that our implementation looks for more than one best centroid or final point \remark{w części teoretycznej jest wspomniany algorytm z przeszukiwaniem $n$ poddrzew}, so a
fixed-size priority queue is used to remember the best candidates. Each time an inner
product of query with a new candidate is calculated, it is added to the queue.
Then, if the queue size is bigger than specified capacity, the smallest product is
removed.

Another optimization was also implemented --- it hinges on fact that the inner product
$q \cdot x$ cannot be bigger than $q \cdot c + ||c - x||$ (a variant of triangle inequality).
Thus, we could precompute distances of each centroid $c$ to each of its children $x$,
and then only try to compute inner product $q \cdot x$ if there is a chance for 
it to exceed the worst candidate so far. In practice however, we found that this
optimization was not very useful --- one of the reasons could be the high space
dimensionality causing most centroid-child distances to be relatively large.\footnote{
Centroid's children can be modeled as random points in a hyperball centered on the
centroid. Somewhat counterintuitively, for a large number of dimensions, 
most of its hypervolume is near its boundary, meaning the average distance of point
to the center is close to its radius.
} In the end, the optimization was disabled as it mostly generated an overhead of a
couple of percent for checking the condition.
